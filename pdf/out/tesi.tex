\documentclass[italian,a4paper, twoside, 12pt]{report}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Palatino Linotype}
    \setmonofont[Mapping=tex-ansi]{InconsolataForPowerline Nerd Font}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=NavyBlue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[a4paper, left=3.5cm, right=3cm, top=2.5cm, bottom=3cm]{geometry}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=italian]{babel}
\else
  \usepackage{polyglossia}
  \setmainlanguage[]{italian}
\fi
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{calc}
\addto\captionsitalian{\renewcommand{\chaptername}{Parte}}
\renewcommand{\baselinestretch}{1.1} 
% \renewcommand\thechapter{\Roman{chapter}}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{subfig}
\usepackage{float}
\floatstyle{plain}
\makeatletter
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\makeatother
\floatname{codelisting}{Listato}
\newcommand*\listoflistings{\listof{codelisting}{Elenco dei Listati}}

\date{}

\begin{document}

\definecolor{unicamblue}{HTML}{002860}

\newgeometry{left=2.25cm,right=2.25cm,top=1cm,bottom=1cm,a4paper}

\begin{titlepage}
    \color{unicamblue}

\begin{tikzpicture}[remember picture, overlay]
    \color{black}
  \draw[line width = 0.5pt] ($(current page.north west) + (1cm,-1cm)$) rectangle ($(current page.south east) + (-1cm,1cm)$);
\end{tikzpicture}

\begin{center}

\vspace{1cm}
\Huge\sc Università degli Studi di Camerino\\
\vspace{3mm}
\huge\bf Scuola di Scienze e Tecnologie\\
\vspace{3mm}
    \LARGE \bf \textsl{Corso di Laurea in Informatica}
\end{center}

\vspace{14mm}

\begin{center}

\includegraphics[width=3.5cm]{img/unicam_logo.pdf}\\

\vspace{1.5cm}
{\Huge{\bf Big Data}}\\
\vspace{5mm}
{\huge{\bf Tecniche e tool di analisi}}\\
\vspace{15mm} {\huge{Elaborato Finale}}

\end{center}

\vspace{25mm}
\par
\noindent

\begin{minipage}[t]{0.47\textwidth}
    \begin{center}
    {\LARGE{\textsl{Laureando}
    \vspace{0.2cm}\\
    \textbf{Emanuele Gentiletti}\\
    \vspace{1.7cm}
    Matricola: \textbf{090150}
    }}

    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\textwidth}
    \begin{center}
    {\LARGE{\textsl{Relatore}
    \vspace{0.2cm}\\
    \bf Prof. Diletta Romana Cacciagrano\\
    }}

    \end{center}
\end{minipage}
\vspace{35mm}
\begin{center}
\rule[0.1cm]{14cm}{0.1mm}\\
\vspace{2mm}
{\Large{\textsl{Anno Accademico 2016/2017}}}
\end{center}
\end{titlepage}
\restoregeometry
\clearpage
\cleardoublepage

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{2}
\tableofcontents
}
\clearpage

\chapter*{Introduzione}\label{introduzione}
\addcontentsline{toc}{chapter}{Introduzione}

Negli ultimi decenni, i Big Data hanno preso piede in modo impetuoso in
una grande varietà di ambiti. Il fenomeno ha avuto un enorme impatto:
settori come medicina, finanza, business analytics e marketing sfruttano
i Big Data per guidare lo sviluppo in modi semplicemente non possibili
prima.

L'innovazione che rende possibili questi risultati è guidata dal
software molto più che dall'hardware. Ci sono stati dei grandi
cambiamenti nel modo di pensare alla computazione e all'organizzazione
dei suoi processi, che hanno portato a risultati notevoli
nell'efficienza di elaborazione di grandi quantità di dati.

Uno dei più importanti fenomeni che hanno portato a questa spinta è
stato lo sviluppo di Hadoop, un framework open source progettato per la
computazione batch di dataset di grandi dimensioni. Utilizzando
un'architettura ben congeniata, Hadoop ha permesso l'analisi in tempi
molto rapidi di interi dataset di dimensioni nell'ordine dei terabyte,
fornendo una capacità di sfruttamento di questi, e conseguentemente un
valore molto più alti.

Una delle conseguenze più importanti di Hadoop è stata una
democratizzazione delle capacità di analisi dei dati:

\begin{itemize}
\tightlist
\item
  Hadoop è sotto licenza Apache, permettendo a chiunque di utilizzarlo a
  scopi commerciali e non;
\item
  Hadoop non richiede hardware costoso ad alta affidabilità, e
  incoraggia l'adozione di macchine più generiche e prone al fallimento
  per il suo uso, che possono essere ottenute a costi inferiori;
\item
  Il design di Hadoop permette la sua esecuzione in cluster di macchine
  eterogenee nel software e nell'hardware che possono essere acquisite
  da diversi rivenditori, un altro fattore che permette l'abbattimento
  dei costi;
\item
  I vari modelli di programmazione in Hadoop hanno in comune
  l'astrazione della computazione distribuita e dei problemi intricati
  che questa comporta, abbassando la barriere in entrata in termini di
  conoscenze e lavoro richiesti per creare programmi che necessitano di
  un altro grado di parallelismo.
\end{itemize}

Questi fattori hanno spinto a una vasta adozione di Hadoop e
dell'ecosistema software che lo circonda, in ambito aziendale e
scientifico. L'adozione di Hadoop, secondo un sondaggio fatto a maggio
2015{[}\protect\hyperlink{ref-hadoop-adoption-survey}{1}{]}, si aggira
al 26\% delle imprese negli Stati Uniti, e si prevede che il mercato
attorno ad Hadoop sorpasserà i 16 miliardi di dollari nel 2020
{[}\protect\hyperlink{ref-hadoop-market-analysis}{2}{]}.

Tutto questo accade in un'ottica in cui la produzione di informazioni
aumenta ad una scala senza precedenti: secondo uno studio di
IDC{[}\protect\hyperlink{ref-digital-univ}{3}{]}, la quantità di
informazioni nell'``Universo Digitale'' ammontava a 4.4 TB nel 2014, e
la sua dimensione stimata nel 2020 è di 44 TB.

Data la presenza di questa vasta quantità di informazioni, lo
sfruttamento efficace di queste è fonte di grandi opportunità. In questo
documento si analizzano le varie tecniche che sono a disposizione per
l'utilizzo effettivo dei Big Data, come queste differiscono tra di loro,
e quali strumenti le mettono a disposizione. Si parlerà inoltre di come
gli strumenti possano essere integrati in sistemi di produzione
esistenti, le possibili architetture di un sistema di questo tipo e come
\ldots{}

La gestione di sistemi per l'elaborazione di Big Data richiede una
configurazione accurata per ottenere affidabilità e fault-tolerance. Pur
sottolineando che l'importanza di questi aspetti non è da sottovalutare,
questa tesi si concentrerà più sul modello computazionale e di
programmazione che gli strumenti offrono.

\chapter{Big Data e Paradigmi di
Elaborazione}\label{big-data-e-paradigmi-di-elaborazione}

Per Big Data si intendono collezioni di dati con caratteristiche tali da
richiedere strumenti innovativi per poterli gestire e analizzare. Uno
dei modelli tradizionali e più popolari per descrivere le
caratteristiche dei Big Data si chiama \textbf{modello delle 3V}. Il
modello identifica i Big Data come collezioni di informazione che
presentano grande abbondanza in una più delle seguenti caratteristiche:

\begin{itemize}
\tightlist
\item
  Il \textbf{volume} delle informazioni, che può aggirarsi dalle decine
  di terabyte per arrivare all'ordine dei petabyte;
\item
  La \textbf{varietà}, intesa come la varietà di \emph{fonti} e di
  \emph{possibili strutturazioni} delle informazioni di interesse;
\item
  La \textbf{velocità} di produzione delle informazioni di interesse.
\end{itemize}

Ognuno dei punti di questo modello deriva da esigenze che vanno ad
accentuarsi andando avanti nel tempo, in particolare:

\begin{itemize}
\item
  Il volume delle collezioni dei dati è aumentato esponenzialmente in
  tempi recenti, con l'avvento dei Social Media, dell'IOT, e degli
  smartphone muniti di molti sensori diversi. Generalizzando, i fattori
  che hanno portato a un grande incremento del volume dei data set sono
  un aumento della generazione automatica di dati da parte dei
  dispositivi e dei contenuti prodotti dagli utenti.
\item
  L'aumento dei dispositivi e dei dati generati dagli utenti portano
  conseguentemente a un aumento delle fonti dei dati, ed essendo queste
  gestite da enti e persone diverse la struttura che le fonti presentano
  difficilmente sarà uniforme, l'una rispetto all'altra. Inoltre,
  l'utilizzo di dati non strutturati rigidamente è prevalente nelle
  tecnologie web (in particolare documenti JSON), che sono spesso un
  obiettivo desiderabile di analisi.
\item
  Si possono fare le stesse considerazioni fatte per il volume dei dati
  per quanto riguarda la velocità. I flussi di dati vengono generati dai
  dispositivi e dagli utenti, che li producono a velocità molto maggiori
  rispetto agli operatori.
\end{itemize}

Per l'elaborazione di dataset con queste caratteristiche sono stati
sviluppati molti strumenti, che usano diversi pattern di elaborazione a
seconda delle esigenze dell'utente e del tipo di dati con cui si ha a
che fare. I modelli di elaborazione più importanti e rappresentativi
sono il \emph{batch processing} e lo \emph{stream processing}.

\section{Batch e Streaming
Processing}\label{batch-e-streaming-processing}

Il batch processing è il pattern di elaborazione generalmente più
efficiente, e consiste nell'elaborazione di un intero dataset in
un'unità di lavoro, per poi ottenere i risultati al termine di questa.

Questo approccio è ottimale quando non c'è una necessità impendente di
avere risultati, ma in alcuni casi è necessario avere i risultati a
disposizione mano a mano che la computazione procede, e il batch
processing non è adatto a questo scopo:

\begin{itemize}
\item
  Le fasi del batch processing richiedono la schedulazione dei lavori da
  parte dell'utente, con un conseguente overhead dovuto alla
  schedulazione in sé o alla configurazione di strumenti automatizzati
  che se ne occupino;
\item
  Non è possibile accedere ai risultati prima del termine del job, che
  può avere una durata eccessiva rispetto alle esigenze
  dell'applicazione o dell'utente.
\end{itemize}

Per use case in cui questi fattori sono rilevanti, lo \textbf{stream
processing} si presta come più adatto. In questo paradigma, i dati da
elaborare vengono ricevuti da stream (nella maggior parte dei casi da
Internet) e vengono processati mano a mano con il loro arrivo. I job in
streaming molto spesso non hanno un termine prestabilito, ma vengono
terminati dall'utente, e i risultati dell'elaborazione possono essere
disponibili mano a mano che l'elaborazione procede, permettendo quindi
un feedback più rapido rispetto ai lavori batch.

\subsection{\texorpdfstring{\emph{Data at Rest} e \emph{Data in
Motion}}{Data at Rest e Data in Motion}}\label{data-at-rest-e-data-in-motion}

I due paradigmi si differenziano anche per il modo in cui i dati sono
disponibili. Il processing batch richiede che l'informazione sia
\emph{data at rest}, ovvero informazioni salvate interamente in un mezzo
di memoria accessibile al programma. I dati di input in una computazione
batch sono determinati all'inizio dell'elaborazione, e non possono
cambiare nel corso di questa. Questo significa che se nuova informazione
arriva nel corso di un job batch, questa non può essere tenuta in conto
nell'elaborazione finale.

Lo \textbf{stream processing}, invece, è progettato per \emph{data in
motion}, dati in arrivo continuo la cui quantità non è fissa a priori. È
possibile utilizzare strumenti di processing in streaming anche per
\emph{data at rest}, rappresentando il dataset come uno stream. Questa
proprietà è desiderabile, perché permette di utilizzare le stesse
applicazioni per l'elaborazione di dati in arrivo dalla rete e quelli
salvati. Come si vedrà, la Kappa architecture, ovvero una possibile
architettura software per l'elaborazione di Big Data, utilizza questa
proprietà per sfruttare un solo paradigma sia per computazioni in
real-time di dati in arrivo da stream, che per i dati salvati
storicamente, permettendo di utilizzare gli stessi tool e interfacce di
programmazione per entrambi i tipi di elaborazione e massimizzare il
riutilizzo di codice.

\begin{longtable}[]{@{}lll@{}}
\caption{Differenze tra elaborazione batch e streaming}\tabularnewline
\toprule
\begin{minipage}[b]{0.27\columnwidth}\raggedright\strut
Caratteristiche\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
Batch\strut
\end{minipage} & \begin{minipage}[b]{0.43\columnwidth}\raggedright\strut
Streaming\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.27\columnwidth}\raggedright\strut
Caratteristiche\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
Batch\strut
\end{minipage} & \begin{minipage}[b]{0.43\columnwidth}\raggedright\strut
Streaming\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Ottimizzazione\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
Alto throughput\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright\strut
Bassa latenza\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Tipo di informazione\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\emph{Data at rest}\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright\strut
\emph{Data in motion} e \emph{Data at rest}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Accesso ai dati\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
Stabilito all'inizio\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright\strut
Dipendente dallo stream\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Accesso ai risultati\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
Fine job\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright\strut
Continuo\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Un esempio di \emph{data at rest} sono i resoconti delle vendite di
un'azienda, su cui si possono cercare pattern per identificare quali
prodotti sono in trend nelle vendite. Per \emph{data in motion} si può
considerare l'invio di dati da parte di sensori IoT o le pubblicazioni
degli utenti nei social media, che sono continui e senza una fine
determinata.

\section{Hadoop e modelli di
elaborazione}\label{hadoop-e-modelli-di-elaborazione}

Nella prossima sezione si discuterà di Hadoop, un progetto nato con
l'intento di affrontare la computazione batch

\hypertarget{hadoop}{\chapter{Hadoop}\label{hadoop}}

Nell'ambito dei Big Data, Hadoop è il perno centrale su cui è basato un
\emph{ecosistema} di tool e tecnologie, tant'è che spesso il termine
Hadoop viene utilizzato per riferirsi all'intero ecosistema di tool e
tecnologie construiti attorno a questo.

La documentazione
ufficiale{[}\protect\hyperlink{ref-hadoop-doc-main}{4}{]} lo descrive
come:

\begin{quote}
\ldots{}un framework che abilita l'elaborazione distribuita di grandi
dataset in cluster di computer utilizzando semplici modelli di
programmazione. \protect\hyperlink{hadoop}{Hadoop} è progettato per
essere scalato da server singoli a migliaia di macchine, dove ognuna di
queste offre computazione e storage locale. Invece di affidarsi
all'hardware per fornire un'alta affidabilità,
\protect\hyperlink{hadoop}{Hadoop} è progettato per rilevare e gestire i
fallimenti {[}delle computazioni{]} a livello applicativo, mettendo a
disposizione un servizio ad alta affidiabilità su cluster di computer
proni al fallimento.
\end{quote}

In questa definizione sono racchiusi dei punti molti importanti:

\begin{itemize}
\item
  \textbf{Semplici modelli di programmazione}

  Hadoop raggiunge molti dei suoi obiettivi fornendo un'interfaccia di
  livello molto alto al programmatore, in modo di potersi assumere la
  responsabilità di concetti complessi e necessari all'efficienza nella
  computazione distribuita, ma che hanno poco a che fare con il problema
  da risolvere in sé (ad esempio, la sincronizzazione di task paralleli
  e lo scambio dei dati tra nodi del sistema distribuito). Questo
  modello \textbf{pone dei limiti alla libertà del programmatore}, che
  deve adeguare la codifica della risoluzione del problema al modello di
  programmazione fornito.
\item
  \textbf{Computazione e storage locale}

  L'ottimizzazione più importante che Hadoop fornisce rispetto
  all'elaborazione dei dati è il risultato dell'unione di due concetti:
  \textbf{distribuzione dello storage} e \textbf{distribuzione della
  computazione}.

  Entrambi sono importanti a prescindere dell'uso particolare che ne fa
  Hadoop: la distribuzione dello storage permette di combinare lo spazio
  fornito da più dispositivi e di farne uso tramite un'unica interfaccia
  logica, e di replicare i dati in modo da poter tollerare guasti nei
  dispositivi. La distribuzione della computazione permette di aumentare
  il grado di parallelizazione nell'esecuzione dei programmi.

  Hadoop unisce i due concetti utilizzando cluster di macchine che hanno
  sia lo scopo di mantenere lo storage, che quello di elaborare i dati.
  Quando Hadoop esegue un lavoro, \textbf{quante più possibili delle
  computazioni richieste vengono eseguite nei nodi che contengono i dati
  da elaborare}. Questo permette di ridurre la latenza di rete,
  minimizzando la quantità di dati che devono essere scambiati tra i
  nodi del cluster. Il meccanismo è trasparente all'utente, a cui basta
  persitere i dati da elaborare nel cluster per usifruirne. Questo
  principio viene definito \textbf{data locality}.
\item
  \textbf{Rack awareness}

  Nel contesto di Hadoop, \emph{rack awareness} si riferisce a delle
  ottimizzazioni sull'utilizzo di banda di rete e sull'affidabilità che
  Hadoop fa basandosi sulla struttura del cluster.

  \begin{figure}
  \def\svgwidth{\linewidth}
  \input{img/hadoop_topology.pdf_tex}
  \label{fig:hadoop-topology}
  \caption{Topologia di rete tipica di un cluster Hadoop.}
  \end{figure}

  Quando configurato per essere \emph{rack aware}, Hadoop considera il
  cluster come un insieme di \emph{rack} che contengono i nodi del
  cluster. Tutti i nodi di un rack sono connessi a uno switch di rete (o
  dispositivo equivalente), e tutti gli switch sono a loro volta
  connessi a uno switch centrale. Questa struttura è mostrata in
  fig.~\ref{fig:hadoop-topology}.

  A partire da questa struttura si può fare un'assunzione importante: la
  comunicazione tra nodi in uno stesso rack è meno onerosa in termini di
  banda rispetto alla comunicazione tra nodi in rack diversi, perché la
  comunicazione può essere commutata tramite un solo switch.

  Quando possibile, Hadoop utilizza questo principio per minimizzare
  l'uso di banda tra nodi del cluster. Come si vedrà, i vari componenti
  di Hadoop fanno uso della configurazione di rete per raggiungere
  diversi risultati.
\item
  \textbf{Scalabilità}

  \textbar{}\textbar{}\textbar{}
\item
  \textbf{Hardware non necessariamente affidabile}

  I cluster di macchine che eseguono Hadoop non hanno particolari
  requisiti di affidabilità rispetto ad hardware consumer. Il framework
  è progettato per tenere in conto dell'alta probabilità di fallimento
  dell'hardware, e per attenuarne le conseguenze, sia dal punto di vista
  dello storage e della potenziale perdita di dati, che da quello della
  perdita di risultati intermedi e parziali nel corso dell'esecuzione di
  lavori computazionalmente costosi. In questo modo l'utente è sgravato
  dal compito generalmente difficile di gestire fallimenti parziali nel
  corso delle computazioni.
\end{itemize}

Hadoop è composto da diversi moduli:

\begin{itemize}
\item
  \textbf{HDFS}, un filesystem distribuito ad alta affidabilità, che
  fornisce replicazione automatica all'interno dei cluster e accesso ad
  alto throughput ai dati
\item
  \textbf{YARN}, un framework per la schedulazione di lavori e per la
  gestione delle risorse all'interno del cluster
\item
  \textbf{MapReduce}, un framework e un modello di programmazione
  fornito da Hadoop per la scrittura di programmi paralleli che
  processano grandi dataset.
\end{itemize}

\hypertarget{installazione-e-configurazione}{\section{Installazione e
Configurazione}\label{installazione-e-configurazione}}

Ogni versione di Hadoop viene distribuita in tarball, una con i
sorgenti, da cui si può eseguire una build manuale, e una binaria, che
può essere estratta e utilizzata così com'è. Per un approccio più
strutturato, sono disponibili repository che forniscono versioni
pacchettizzate di Hadoop, come il PPA per
Ubuntu{[}\protect\hyperlink{ref-hadoop-ppa}{5}{]} e i pacchetti AUR per
Arch Linux{[}\protect\hyperlink{ref-hadoop-aur}{6}{]}.

Ci sono anche distribuzioni di immagini virtuali Linux create
appositamente con lo scopo di fornire un ambiente preconfigurato con
Hadoop e vari componenti del suo ecosistema. I due ambienti più
utilizzati di questo tipo sono Cloudera QuickStart e HortonWorks
Sandbox, disponibili per VirtualBox, VMWare e Docker. Gli esempi di
questo documento sono eseguiti prevalentemente da Arch Linux e dalla
versione Docker di HortonWorks Sandbox.

Hadoop è configurabile tramite file XML, che si trovano rispetto alla
cartella d'installazione in \texttt{etc/hadoop}. Ogni componente di
Hadoop (HDFS, MapReduce, Yarn) ha un file di configurazione apposito che
contiene impostazioni relative al componente stesso, mentre un altro
file di configurazione contiene proprietà comuni a tutti i componenti.

\begin{longtable}[]{@{}llll@{}}
\caption{Nomi dei file di configurazione per i componenti di
Hadoop}\tabularnewline
\toprule
\begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
Comuni\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
HDFS\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
YARN\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\raggedright\strut
MapReduce\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
Comuni\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
HDFS\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
YARN\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\raggedright\strut
MapReduce\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.22\columnwidth}\raggedright\strut
\texttt{core-site.xml}\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright\strut
\texttt{hdfs-site.xml}\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright\strut
\texttt{yarn-site.xml}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright\strut
\texttt{mapred-site.xml}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{codelisting}

\caption{Esempio di file di configurazione personalizzato di Hadoop.}

\hypertarget{lst:hadoop-conf-example}{\label{lst:hadoop-conf-example}}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{<?xml}\NormalTok{ version="1.0"}\KeywordTok{?>}
\KeywordTok{<configuration>}
    \KeywordTok{<property>}
        \KeywordTok{<name>}\NormalTok{fs.defaultFS}\KeywordTok{</name>}
        \KeywordTok{<value>}\NormalTok{hdfs://namenode/}\KeywordTok{</value>}
    \KeywordTok{</property>}
    \KeywordTok{<property>}
        \KeywordTok{<name>}\NormalTok{mapreduce.framework.name}\KeywordTok{</name>}
        \KeywordTok{<value>}\NormalTok{yarn}\KeywordTok{</value>}
    \KeywordTok{</property>}
    \KeywordTok{<property>}
        \KeywordTok{<name>}\NormalTok{yarn.resourcemanager.address}\KeywordTok{</name>}
        \KeywordTok{<value>}\NormalTok{resourcemanager:8032}\KeywordTok{</value>}
    \KeywordTok{</property>}
\KeywordTok{</configuration>}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

È anche possibile selezionare un'altra cartella da cui prendere i file
di configurazione, impostandola come valore della variabile d'ambiente
\texttt{HADOOP\_CONF\_DIR}. Un approccio comune alla modifica dei file
di configurazione consiste nel copiare il contenuto di
\texttt{etc/hadoop} in un'altra posizione, specificare questa in
\texttt{HADOOP\_CONF\_DIR} e fare le modifiche nella nuova cartella. In
questo modo si evita di modificare l'albero d'installazione di Hadoop.

Per molti degli eseguibili inclusi in Hadoop, è anche possibile
specificare un file che contiene ulteriori opzioni di configurazione,
che possono sovrascrivere quelle in \texttt{HADOOP\_CONF\_DIR} tramite
lo switch \texttt{-conf}.

\hypertarget{esecuzione-di-software-in-hadoop}{\subsection{Esecuzione di
software in Hadoop}\label{esecuzione-di-software-in-hadoop}}

I programmi che sfruttano il runtime di Hadoop sono generalmente
sviluppati in Java (o in un linguaggio che ha come target di
compilazione la JVM), e vengono avviati tramite l'eseguibile
\texttt{hadoop}. L'eseguibile richiede che siano specificati il
classpath del programma, e una classe contente un metodo \texttt{main}
che si desidera eseguire (analogo all'entry point dei programmi Java).

Il classpath può essere specificato tramite la variabile d'ambiente
\texttt{HADOOP\_CLASSPATH}, che può essere il percorso di una directory
o di un file \texttt{jar}. La classe con il metodo \texttt{main} da
invocare viene messa tra i parametri del comando \texttt{hadoop},
seguita dagli argomenti che si vogliono passare in \texttt{args{[}{]}}.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Volendo eseguire il seguente programma in Hadoop:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{public} \KeywordTok{class}\NormalTok{ SayHello \{}
    \KeywordTok{public} \DataTypeTok{static} \DataTypeTok{void} \FunctionTok{main}\NormalTok{(}\BuiltInTok{String}\NormalTok{ args[]) \{}
        \BuiltInTok{System}\NormalTok{.}\FunctionTok{out}\NormalTok{.}\FunctionTok{println}\NormalTok{(}\StringTok{"Hello "}\NormalTok{ + args[}\DecValTok{0}\NormalTok{] + }\StringTok{"!"}\NormalTok{);}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

  Lo si può compilare e pacchettizzare in un file \texttt{jar}, per poi
  utilizzare i seguenti comandi:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{$}\ExtensionTok{~} \BuiltInTok{export} \VariableTok{HADOOP_CLASSPATH=}\NormalTok{say_hello.jar }
\NormalTok{$}\ExtensionTok{~} \ExtensionTok{hadoop}\NormalTok{ SayHello Josh}

\ExtensionTok{Hello}\NormalTok{ Josh!}
\end{Highlighting}
\end{Shaded}
\item
  In alternativa, si può eseguire il comando \texttt{hadoop\ jar}, e
  specificare il file \texttt{jar} direttamente nei suoi argomenti:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{$}\ExtensionTok{~} \ExtensionTok{hadoop}\NormalTok{ jar say_hello.js SayHello Josh}

\ExtensionTok{Hello}\NormalTok{ Josh!}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

In generale, i programmi eseguiti in Hadoop fanno uso della sua libreria
client. La libreria fornisce accesso al package
\texttt{org.apache.hadoop}, che contiene le API necessarie per
interagire con Hadoop. Non è necessario che la libreria client si trovi
nel classpath finale, in quanto il runtime di Hadoop fornisce le classi
della libreria a runtime.

Per gestire le dipendenze e la pacchettizzazione dei programmi per
Hadoop è pratico utilizzare un tool di gestione delle build. Negli
esempi in questo documento si utilizza Maven a questo scopo, che
permette di specificare le proprietà di un progetto, tra cui le sue
dipendenze, in un file XML chiamato POM (Project Object Model). A
partire dal POM, Maven è in grado di scaricare automaticamente le
dipendenze del progetto, e di pacchettizzarle correttamente negli
artefatti \texttt{jar} a seconda della configurazione fornita.

\begin{codelisting}

\caption{Un esempio semplificato di un POM per il programma SayHello.}

\hypertarget{lst:pom_example}{\label{lst:pom_example}}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\KeywordTok{<?xml}\NormalTok{ version="1.0" encoding="UTF-8"}\KeywordTok{?>}
\KeywordTok{<project}\OtherTok{ xmlns=}\StringTok{"..."} \KeywordTok{>}
    \KeywordTok{<modelVersion>}\NormalTok{4.0.0}\KeywordTok{</modelVersion>}

    \KeywordTok{<groupId>}\NormalTok{com.example}\KeywordTok{</groupId>}
    \KeywordTok{<artifactId>}\NormalTok{say_hello}\KeywordTok{</artifactId>}
    \KeywordTok{<version>}\NormalTok{1.0}\KeywordTok{</version>}

    \KeywordTok{<dependencies>}

        \CommentTok{<!-- Libreria client di Hadoop -->}
        \KeywordTok{<dependency>}
            \KeywordTok{<groupId>}\NormalTok{org.apache.hadoop}\KeywordTok{</groupId>}
            \KeywordTok{<artifactId>}\NormalTok{hadoop-client}\KeywordTok{</artifactId>}
            \KeywordTok{<version>}\NormalTok{2.8.1}\KeywordTok{</version>}
            \KeywordTok{<scope>}\NormalTok{provided}\KeywordTok{</scope>}
        \KeywordTok{</dependency>}

    \KeywordTok{</dependencies>}
\KeywordTok{</project>}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

Maven è in grado di gestire correttamente la dipendenza della libreria
client di Hadoop, attraverso un meccanismo chiamato \emph{dependency
scope}. Per ogni dipendenza è possibile specificare una proprietà
\emph{scope}, che indica in che modo la dipendenza debba essere gestita
a tempo di build (in particolar modo, se debba essere inclusa nel
classpath). Se non specificato, lo scope è impostato a \texttt{compile},
che indica che la dipendenza è resa disponibile nel classpath
dell'artefatto. Per gestire correttamente la dipendenza dalla libreria
client di Hadoop, è opportuno impostare lo scope della dipendenza a
\texttt{provided}, che indica che le classi della libreria sono fornite
dal container in cui è eseguito il programma.

\section{HDFS}\label{hdfs}

HDFS è un filesystem distribuito che permette l'accesso ad alto
throughput ai dati. HDFS è scritto in Java, e viene eseguito nello
userspace. Lo storage dei dati passa per il filesystem del sistema che
lo esegue.

I dati contenuti in HDFS sono organizzati in unità logiche chiamate
\emph{blocchi}, come è comune nei filesystem. I blocchi di un singolo
file possono essere distribuiti all'interno di più macchine all'interno
del cluster, permettendo di avere file più grandi della capacità di
storage di ogni singola macchina nel cluster. Rispetto ai filesystem
comuni la dimensione di un blocco è molto più grande, 128 MB di default.
La ragione per cui HDFS utilizza blocchi così grandi è minimizzare il
costo delle operazioni di seek, dato il fatto che se i file sono
composti da meno blocchi, si rende necessario trovare l'inizio di un
blocco un minor numero di volte. Questo approccio riduce anche la
frammentazione dei dati, rendendo più probabile che questi vengano
scritti contiguamente all'interno della macchina\footnote{Non è
  possibile essere certi della contiguità dei dati, perché HDFS non è
  un'astrazione diretta sulla scrittura del disco, ma sul filesystem del
  sistema operativo che lo esegue. Per cui la frammentazione effettiva
  dipende da come i dati vengono organizzati dal filesystem sottostante.}.

Il blocco, inoltre, è un'astrazione che si presta bene alla replicazione
dei dati nel filesystem all'interno del cluster: per replicare i dati,
come si vedrà, si mette uno stesso blocco all'interno di più macchine
nel cluster.

HDFS è basato sulla specifica POSIX, ma non la implementa in modo
rigido: tralasciare alcuni requisiti di conformità alla specifica
permette ad HDFS di ottenere prestazioni e affidabilità migliori, come
verrà descritto in seguito.

\subsection{Principi architetturali}\label{principi-architetturali}

\begin{figure}
\centering
\includegraphics{img/hdfsarchitecture.png}
\caption{Schema di funzionamento dell'architettura di HDFS}
\end{figure}

La documentazione di Hadoop descrive i seguenti come i principi
architetturali alla base della progettazione di HDFS:

\begin{itemize}
\item
  \textbf{Fallimento hardware come regola invece che come eccezione}

  Un sistema che esegue HDFS è composto da molti componenti, con
  probabilità di fallimento non triviale. Sulla base di questo
  principio, HDFS da' per scontato che \textbf{ci sia sempre un numero
  di componenti non funzionanti}, e si pone di rilevare errori e guasti
  e di fornire un recupero rapido e automatico da questi.

  Il meccanismo principale con cui HDFS raggiunge questo obiettivo è la
  replicazione: in un cluster, ogni blocco di cui un file è composto è
  replicato in più macchine (3 di default). Se un blocco non è
  disponibile in una macchina, o se non supera i controlli di integrità,
  una sua copia può essere letta da un'altra macchina in modo
  trasparente per il client.

  Il numero di repliche per ogni blocco è configurabile, e ci sono più
  criteri con cui viene deciso in quali macchine il blocco viene
  replicato, principalmente orientati al risparmio di banda di rete.
\item
  \textbf{Modello di coerenza semplice}

  Per semplificare l'architettura generale, HDFS fa delle assunzioni
  specifiche sul tipo di dati che vengono salvati in HDFS e pone dei
  limiti su come l'utente possa lavorare sui file. In particolare,
  \textbf{non è possibile modificare arbitrariamente file già
  esistenti}, e le modifiche devono limitarsi a operazioni di
  troncamento e di aggiunta a fine file. Queste supposizioni permettono
  di semplificare il modello di coerenza, perché i blocchi di dati, una
  volta scritti, possono essere considerati immutabili, evitando una
  considerevole quantità di problemi in un ambiente dove i blocchi di
  dati sono replicati in più posti:

  \begin{itemize}
  \item
    Per ogni modifica a un blocco di dati, bisognerebbe verificare quali
    altre macchine contengono il blocco, e rieseguire la modifica (o
    rireplicare il blocco modificato) in ognuna di queste.
  \item
    Queste modifiche dovrebbero essere fatte in modo atomico, o
    richieste di lettura su una determinata replica di un blocco invece
    che in un'altra potrebbe portare a risultati inconsistenti o non
    aggiornati.
  \end{itemize}

  Le limitazioni che Hadoop impone sono ragionevoli per lo use-case per
  cui HDFS è progettato, caratterizzato da grandi dataset che vengono
  copiati nel filesystem e letti in blocco.
\item
  \textbf{Dataset di grandi dimensioni}

  I filesystem distribuiti sono generalmente necessari per aumentare la
  capacità di storage disponibile oltre quella di una singola macchina.
  La distribuzione di HDFS, assieme alla grande dimensione dei blocchi
\item
  \textbf{Accesso in streaming}

  HDFS predilige l'accesso ai dati in streaming, per permettere ai
  lavori batch di essere eseguiti con grande efficienza. Questo
  approccio va a discapito del tempo di latenza della lettura dei file,
  ma permette di avere un throughput in lettura molto vicino ai tempi di
  lettura del disco.
\item
  \textbf{Portabilità su piattaforme software e hardware eterogenee}

  HDFS è scritto in Java, ed è portabile in tutti i sistemi che ne
  supportano il runtime.
\end{itemize}

L'architettura di HDFS è di tipo master/slave, dove un nodo centrale,
chiamato \textbf{NameNode}, gestisce i metadati e la struttura del
filesystem, mentre i nodi slave, chiamati \textbf{DataNode}, contengono
i blocchi di cui file sono composti. Tipicamente, viene eseguita
un'istanza del software del DataNode per macchina del cluster, e una
macchina dedicata esegue il NameNode.

I \emph{client} del filesystem interagiscono sia con il NameNode che con
i DataNode per l'accesso ai file. La comunicazione tra il client e i
nodi avviene tramite socket TCP ed è coordinata dal NameNode, che
fornisce ai client tutte le informazioni sul filesystem e su quali nodi
contengono i DataBlock dei file richiesti.

\subsection{Comunicare con HDFS}\label{comunicare-con-hdfs}

Hadoop fornisce tool e librerie che possono agire da client nei
confronti di HDFS. Il tool più diretto è la CLI, accessibile nelle
macchine in cui è installato Hadoop tramite il comando
\texttt{hadoop\ fs}.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{%}\NormalTok{ hadoop fs -help}
\ExtensionTok{Usage}\NormalTok{: hadoop fs [generic options]}
\NormalTok{    [}\ExtensionTok{-appendToFile} \OperatorTok{<}\NormalTok{localsrc}\OperatorTok{>}\NormalTok{ ... }\OperatorTok{<}\NormalTok{dst}\OperatorTok{>}\NormalTok{]}
\NormalTok{    [}\ExtensionTok{-cat}\NormalTok{ [-ignoreCrc] }\OperatorTok{<}\NormalTok{src}\OperatorTok{>}\NormalTok{ ...]}
\NormalTok{    [}\ExtensionTok{-checksum} \OperatorTok{<}\NormalTok{src}\OperatorTok{>}\NormalTok{ ...]}
\NormalTok{    [}\ExtensionTok{-chgrp}\NormalTok{ [-R] GROUP PATH...]}
\NormalTok{    [}\ExtensionTok{-chmod}\NormalTok{ [-R] }\OperatorTok{<}\NormalTok{MODE[,MODE]... }\KeywordTok{|} \ExtensionTok{OCTALMODE}\OperatorTok{>}\NormalTok{ PATH...]}
\NormalTok{    [}\ExtensionTok{-chown}\NormalTok{ [-R] [OWNER][:[GROUP]] PATH...]}
\NormalTok{    [}\ExtensionTok{-copyFromLocal}\NormalTok{ [-f] [-p] [-l] [-d] }\OperatorTok{<}\NormalTok{localsrc}\OperatorTok{>}\NormalTok{ ... }\OperatorTok{<}\NormalTok{dst}\OperatorTok{>}\NormalTok{]}
\NormalTok{    [}\ExtensionTok{-copyToLocal}\NormalTok{ [-f] [-p] [-ignoreCrc] [-crc] }\OperatorTok{<}\NormalTok{src}\OperatorTok{>}\NormalTok{ ... }\OperatorTok{<}\NormalTok{localdst}\OperatorTok{>}\NormalTok{]}
\NormalTok{    [}\ExtensionTok{-count}\NormalTok{ [-q] [-h] [-v] [-t [}\OperatorTok{<}\NormalTok{storage type}\OperatorTok{>}\NormalTok{]] [-u] [-x] }\OperatorTok{<}\NormalTok{path}\OperatorTok{>}\NormalTok{ ...]}
\NormalTok{    [}\ExtensionTok{-cp}\NormalTok{ [-f] [-p }\KeywordTok{|} \ExtensionTok{-p}\NormalTok{[topax]] [-d] }\OperatorTok{<}\NormalTok{src}\OperatorTok{>}\NormalTok{ ... }\OperatorTok{<}\NormalTok{dst}\OperatorTok{>}\NormalTok{]}
\ExtensionTok{...}
\end{Highlighting}
\end{Shaded}

La CLI fornisce alcuni comandi comuni nei sistemi POSIX, come
\texttt{cp}, \texttt{rm}, \texttt{mv}, \texttt{ls} e \texttt{chown}, e
altri che riguardano specificamente HDFS, come \texttt{copyFromLocal} e
\texttt{copyToLocal}, utili a trasferire dati tra la macchina su cui si
opera e il filesystem.

I comandi richiedono l'URI che identifica l'entità su cui si vuole
operare. Per riferirsi a una risorsa all'interno di un'istanza di HDFS,
si usa l'URI del namenode, con schema \texttt{hdfs}\footnote{Hadoop è
  abbastanza generale da poter lavorare con diversi filesystem, con lo
  schema definisce il protocollo di comunicazione, che non deve essere
  necessariamente \texttt{hdfs}. Ad esempio, un URI con schema
  \texttt{file} si riferisce al filesystem locale, e le operazioni
  eseguite su URI che utilizzano questo schema vengono effettuate sulla
  macchina dove viene eseguito il comando. Questo approccio può essere
  adatto nella fase di testing dei programmi, ma nella maggior parte dei
  casi è comunque desiderabile lavorare su un filesystem distribuito
  adeguato alla gestione dei Big Data, e un'alternativa ad HDFS degna di
  nota è MapR-FS{[}\protect\hyperlink{ref-mapr-fs}{7}{]}.}, e con il
path corrispondente al percorso della risorsa nel filesystem. Ad
esempio, è possibile creare una cartella \texttt{foo} all'interno della
radice del filesystem con il seguente comando:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{hadoop}\NormalTok{ fs -mkdir hdfs://localhost:8020/foo}
\end{Highlighting}
\end{Shaded}

Per diminuire la verbosità dei comandi è possibile utilizzare percorsi
relativi, e specificare l'opzione \texttt{dfs.defaultFS} nella
configurazione di Hadoop all'URI del filesystem ai cui i percorsi
relativi si riferiscono. In questo modo, si può accorciare l'esempio
precedente a:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{hadoop}\NormalTok{ fs -mkdir foo}
\end{Highlighting}
\end{Shaded}

Ad esempio, data la seguente cartella:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[}\ExtensionTok{root@sandbox}\NormalTok{ example_data]# ls}
\ExtensionTok{example1.txt}\NormalTok{  example2.txt  example3.txt}
\end{Highlighting}
\end{Shaded}

Si possono copiare i file dalla cartella locale della macchina al
filesystem distribuito con il seguente comando:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[}\ExtensionTok{root@sandbox}\NormalTok{ example_data]# hadoop fs -copyFromLocal example*.txt /example}
\end{Highlighting}
\end{Shaded}

Per verificare che l'operazione sia andata a buon fine, si può ottenere
un listing della cartella in cui si sono trasferiti i file con il
comando \texttt{ls}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[}\ExtensionTok{root@sandbox}\NormalTok{ example_data]# hadoop fs -ls /example}
\ExtensionTok{Found}\NormalTok{ 3 items}
\ExtensionTok{-rw-r--r--}\NormalTok{   1 root hdfs         70 2017-06-30 03:58 /example/example1.txt}
\ExtensionTok{-rw-r--r--}\NormalTok{   1 root hdfs         39 2017-06-30 03:58 /example/example2.txt}
\ExtensionTok{-rw-r--r--}\NormalTok{   1 root hdfs         43 2017-06-30 03:58 /example/example3.txt}
\end{Highlighting}
\end{Shaded}

Il listing è molto simile a quello ottenibile su sistemi Unix. Una
differenza importante è la seconda colonna, che non mostra il numero di
hard link al file nel filesystem\footnote{Non è necessario mostrare i
  link dei file, perché HDFS correntemente non li supporta.}, ma il
numero di repliche che HDFS ha a disposizione del file, in questo caso
una per file. Il numero di repliche fatte da HDFS può essere impostato
settando il fattore di replicazione di default, che per Hadoop in
modalità distribuita è 3 di default. Si può anche cambiare il numero di
repliche disponibili per determinati file, utilizzando il comando
\texttt{hdfs\ dfs}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[}\ExtensionTok{root@sandbox}\NormalTok{ ~]# hdfs dfs -setrep 2 /example/example1.txt}
\ExtensionTok{Replication}\NormalTok{ 2 set: /example/example1.txt}
\NormalTok{[}\ExtensionTok{root@sandbox}\NormalTok{ ~]# hadoop fs -ls /example}
\ExtensionTok{Found}\NormalTok{ 3 items}
\ExtensionTok{-rw-r--r--}\NormalTok{   2 root hdfs         70 2017-06-30 03:58 /example/example1.txt}
\ExtensionTok{-rw-r--r--}\NormalTok{   1 root hdfs         39 2017-06-30 03:58 /example/example2.txt}
\ExtensionTok{-rw-r--r--}\NormalTok{   1 root hdfs         43 2017-06-30 03:58 /example/example3.txt}
\end{Highlighting}
\end{Shaded}

HDFS è anche accessibile tramite \emph{HDFS Web Interface}, un tool che
fornisce informazioni sullo stato generale del filesystem e sul suo
contenuto. Ci sono anche tool di amministrazione di cluster Hadoop che
offrono GUI web più avanzate di quella fornita di default da HDFS. Due
esempi sono Cloudera Manager e Apache Ambari, che offrono un file
manager lato web con cui è possibile interagire in modo più semplice,
permettendo anche a utenti in ambito meno tecnico di lavorare con il
filesystem.

\begin{figure}
\centering
\includegraphics{img/ambari_hdfs.png}
\caption{Screenshot del file manager HDFS incluso in Ambari}
\end{figure}

Un'altra interfaccia importante ad HDFS è l'API \texttt{FileSystem} di
Hadoop, che permette un accesso programmatico da linguaggi per JVM a
tutte le funzioni del filesystem. L'API è generale, in modo che possa
essere utilizzata con filesystem diversi da HDFS.

Per linguaggi che non supportano interfacce Java, esiste
un'implementazione in C chiamata \texttt{libhdfs}, che si appoggia sulla
Java Native Interface per esporre l'API di Hadoop.

Esistono poi progetti che permettono il montaggio di HDFS in un
filesystem locale. Alcune di queste implementazioni sono basate su FUSE,
mentre altre su NFS Gateway. Questo metodo di accesso permette
l'utilizzo di utilità native del sistema in uso in HDFS.

\subsection{NameNode}\label{namenode}

Il NameNode è il riferimento centrale per i metadati del filesystem nel
cluster, il che vuol dire che se il NameNode non è disponibile il
filesystem non è accessibile. Questo rende il NameNode un \emph{single
point of failure} del sistema, e per questa ragione HDFS mette a
disposizione dei meccanismi per attenutare l'indisponibilità del sistema
in caso di non reperibilità del NameNode, e per assicurare che lo stato
del filesystem possa essere recuperato a partire dal NameNode.

Il NameNode è anche il nodo a cui i client si connettono alla lettura
del file. La connessione ha il solo scopo di fornire le informazioni sui
DataNode che contengono i dati effettivi del file. I dati di un file non
passano mai per il NameNode.

Tuttavia, il NameNode non salva persistentemente le informazioni sulle
posizioni dei blocchi, che vengono invece mantenute dai DataNode. Perché
il NameNode possa avere in memoria le informazioni sui file necessarie
per essere operativo, questo deve ricevere le liste dei blocchi in
possesso dei DataNode, in messaggi chiamati \textbf{block report}. Non è
necessario che il DataNode conosca la posizione di tutti i blocchi sin
dall'inizio, ma basta che per ogni blocco conosca la posizione di un
numero minimo di repliche, determinato da un'opzione chiamata
\texttt{dfs.replication.min.replicas}, di default 1.

Questa procedura avviene quando il NameNode si trova in uno stato
chiamato \textbf{safe mode}.

\subsubsection{\texorpdfstring{\emph{Namespace image} ed \emph{edit
log}}{Namespace image ed edit log}}\label{namespace-image-ed-edit-log}

Le informazioni sui metadati del sistema vengono salvate nello storage
del NameNode in due posti, la \emph{\textbf{namespace image}} e
l'\emph{\textbf{edit log}}. La \emph{namespace image} è uno snapshot
dell'intera struttura del filesystem, mentre l'\emph{edit log} è un
elenco di operazioni eseguite nel filesystem a partire dalla
\emph{namespace image}. Partendo dalla \emph{namespace image} e
applicando le operazioni registrate nell'\emph{edit log}, è possibile
risalire allo stato attuale del filesystem. Il NameNode ha una
rappresentazione dello stato del filesystem anche nella memoria
centrale, che viene utilizzata per servire le richieste di lettura.

Quando HDFS riceve una richiesta che richiede la modifica dei metadati,
il NameNode esegue le seguenti operazioni:

\begin{enumerate}
\tightlist
\item
  registra la transazione nell'\emph{edit log}
\item
  aggiorna la rappresentazione del filesystem in memoria
\item
  passa all'operazione successiva.
\end{enumerate}

La ragione per cui i cambiamenti dei metadati vengono registrati
nell'\emph{edit log} invece che nella \emph{namespace image} è la
velocità di scrittura: scrivere ogni cambiamento del filesystem mano a
mano che avviene nell'immagine sarebbe lento, dato che questa può avere
dimensioni nell'ordine dei gigabyte. Il NameNode esegue un \emph{merge}
dell'\emph{edit log} e della \emph{namespace image} a ogni suo avvio,
portando lo stato attuale dell'immagine al pari di quello del
filesystem.

Dato che la dimensione dell'\emph{edit log} può diventare notevole, è
utile eseguire l'operazione di \emph{merge} al raggiungimento di una
soglia di dimensione del log. Questa operazione è computazionalmente
costosa, e se fosse eseguita dal NameNode potrebbe interferire con la
sua operazione di routine.

Per evitare interruzioni nel NameNode, il compito di eseguire
periodicamente il \emph{merge} dell'\emph{edit log} è affidato a
un'altra entità, il \textbf{Secondary NameNode}. Il Secondary NameNode
viene solitamente eseguito su una macchina differente, dato che richiede
un'unità di elaborazione potente e almeno la stessa memoria del NameNode
per eseguire l'operazione di merge.

\subsubsection{\texorpdfstring{Avvio del NameNode e \emph{Safe
Mode}}{Avvio del NameNode e Safe Mode}}\label{avvio-del-namenode-e-safe-mode}

Prima di essere operativo, il NameNode deve eseguire alcune operazioni
di startup, tra cui attendere di aver ricevuto i block report dai
DataNode in modo da conoscere le posizioni dei blocchi. Durante queste
operazioni, il NameNode si trova in uno stato chiamato \emph{safe mode},
in cui sono permesse unicamente operazioni che accedono ai metadati del
filesystem, e tentativi di lettura e scrittura di file falliscono. Prima
di poter permettere l'accesso completo, il NameNode ha bisogno di
ricevere le informazioni sui blocchi da parte dei DataNode.

Per ricapitolare, al suo avvio, il NameNode effettua il merge della
\emph{namespace image} con l'\emph{edit log}. Al termine
dell'operazione, il risultato del merge viene salvato come la nuova
\emph{namespace image}. Il Secondary NameNode non viene coinvolto in
questo primo merge.

Prima di uscire dalla safe mode, il NameNode attende di avere abbastanza
informazioni da poter accedere a un numero minimo di repliche di ogni
blocco. A questo punto il NameNode esce dalla safe mode.

Si possono utilizzare dei comandi per verificare lo stato, attivare e
disattivare la safe mode.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{bash-4.1}\NormalTok{$ hdfs dfsadmin -safemode get}
\ExtensionTok{Safe}\NormalTok{ mode is OFF}
\ExtensionTok{bash-4.1}\NormalTok{$ hdfs dfsadmin -safemode enter}
\ExtensionTok{Safe}\NormalTok{ mode is ON}
\ExtensionTok{bash-4.1}\NormalTok{$ hdfs dfsadmin -safemode leave}
\ExtensionTok{Safe}\NormalTok{ mode is OFF}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{img/hdfs-web-startup.png}
\caption{Lo stato dello startup di un'istanza di HDFS, mostrata da HDFS
Web Interface.}
\end{figure}

\subsection{Processo di lettura di file in
HDFS}\label{processo-di-lettura-di-file-in-hdfs}

\begin{figure}
\centering
\includegraphics{img/hdfs-file-read.png}
\caption{Diagramma delle operazioni eseguite nella lettura di un file in
HDFS{[}\protect\hyperlink{ref-hadoop-guide-hdfs-file-read}{8}{]}}
\end{figure}

Per avere un quadro completo del funzionamento di HDFS, è utile
osservare come avvenga il processo di lettura di un file. In questa
sezione si prende in esame un programma di esempio che utilizza le API
\texttt{FileSystem} di Hadoop per reimplementare una versione
semplificata del comando \texttt{cat}, per poi esaminare come le
operazioni specificate nel programma vengano effettivamente portate a
termine in un'istanza di HDFS.

\begin{codelisting}

\caption{Programma di esempio che reimplementa il comando \texttt{cat}.}

\hypertarget{lst:hdfs-cat}{\label{lst:hdfs-cat}}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\KeywordTok{import}\ImportTok{ java.io.InputStream;}
\KeywordTok{import}\ImportTok{ java.net.URI;}

\KeywordTok{import}\ImportTok{ org.apache.hadoop.conf.Configuration;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.fs.FileSystem;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.fs.Path;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.io.IOUtils;}

\KeywordTok{public} \KeywordTok{class}\NormalTok{ MyCat \{}

    \KeywordTok{public} \DataTypeTok{static} \DataTypeTok{void} \FunctionTok{main}\NormalTok{(}\BuiltInTok{String}\NormalTok{ args[]) }\KeywordTok{throws} \BuiltInTok{Exception}\NormalTok{ \{}

        \BuiltInTok{String}\NormalTok{ source = args[}\DecValTok{0}\NormalTok{];}
        \BuiltInTok{Configuration}\NormalTok{ conf = }\KeywordTok{new} \BuiltInTok{Configuration}\NormalTok{();}

        \KeywordTok{try}\NormalTok{(}
\NormalTok{            FileSystem sourcefs = FileSystem.}\FunctionTok{get}\NormalTok{(}\BuiltInTok{URI}\NormalTok{.}\FunctionTok{create}\NormalTok{(source), conf);}
            \BuiltInTok{InputStream}\NormalTok{ in = sourcefs.}\FunctionTok{open}\NormalTok{(}\KeywordTok{new} \FunctionTok{Path}\NormalTok{(source))}
\NormalTok{        ) \{}
\NormalTok{            IOUtils.}\FunctionTok{copyBytes}\NormalTok{(in, }\BuiltInTok{System}\NormalTok{.}\FunctionTok{out}\NormalTok{, }\DecValTok{4096}\NormalTok{, }\KeywordTok{false}\NormalTok{);}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

La reimplementazione del programma \texttt{cat} utilizza il primo
parametro della linea di comando per ricevere l'URI del file che si
vuole stampare nello standard output. L'URI deve contenere il percorso
di rete del filesystem HDFS, ed essere quindi del formato
\texttt{hdfs://{[}indirizzo\ o\ hostname\ del\ namenode{]}/{[}path\ del\ file{]}}.
Di seguito vengono spiegati i passi eseguiti dal programma. Quando non
qualificato, l'identificativo \texttt{hadoop} si riferisce al package
Java \texttt{org.apache.hadoop}.

\begin{enumerate}
\item
  Si crea un oggetto \texttt{hadoop.conf.Configuration}. Gli oggetti
  \texttt{Configuration} forniscono l'accesso ai parametri di
  configurazione di Hadoop (impostati in file XML, come descritto in
  \protect\hyperlink{installazione-e-configurazione}{Installazione e
  Configurazione}).
\item
  Si ottiene un riferimento \texttt{sourcefs} a un
  \texttt{hadoop.fs.FileSystem} (dichiarato come interfaccia Java), che
  fornisce le API che verranno usate per leggere e manipolare il
  filesystem. Il riferimento viene ottenuto tramite il metodo statico
  \texttt{FileSystem.get(URI\ source,\ Configuration\ conf)}, che
  richiede un URI che possa essere utilizzato per risalire a quale
  filesystem si vuole accedere. Un overload di \texttt{FileSystem.get}
  permette di specificare solo l'oggetto \texttt{Configuration}, e
  ottiene le informazioni sul filesystem da aprire dalla proprietà di
  configurazione \texttt{dfs.defaultFS}.
\item
  Si apre il file il lettura, chiamando
  \texttt{sourcefs.open(Path\ file)}. Il metodo restituisce un oggetto
  di tipo \texttt{hadoop.fs.FSDataInputStream}, una sottoclasse di
  \texttt{java.io.InputStream} che supporta anche l'accesso a punti
  arbitrari del file. In questo case l'oggetto è utilizzato per leggere
  il file sequenzialmente, e il suo riferimento viene salvato nella
  variabile \texttt{InputStream\ in}.
\item
  Si copiano i dati dallo stream \texttt{in} a \texttt{System.out}, di
  fatto stampando i dati nella console. Questa operazione è eseguita
  tramite il metodo
  \texttt{hadoop.io.IOUtils.copyBytes(InputStream\ in,\ OutputStream\ out,\ int\ bufSize,\ bool\ closeStream)}.
  Il metodo copia i dati da uno stream d'ingresso a uno d'uscita, e non
  ha funzioni specifiche rispetto ad Hadoop, ma viene fornito per la
  mancanza di un meccanismo simile in Java.
\item
  Lo stream e l'oggetto \texttt{FileSystem} vengono chiusi. L'operazione
  avviene implicitamente utilizzando il costrutto try-with-resources di
  Java.
\end{enumerate}

L'esecuzione del programma dà il seguente output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$}\ExtensionTok{~} \ExtensionTok{hadoop}\NormalTok{ MyCat hdfs://sandbox.hortonworks.com:8020/example/example1.txt}
\ExtensionTok{This}\NormalTok{ is the first example file}
\end{Highlighting}
\end{Shaded}

Nel caso di un URI con schema HDFS, l'istanza concreta di
\texttt{FileSystem} che viene restituita da \texttt{FileSystem.get} è di
tipo \texttt{DistributedFileSystem}, che contiene le funzionalità
necessarie a comunicare con HDFS. Con uno schema diverso (ad esempio
\texttt{file}), l'istanza concreta di \texttt{FileSystem} cambia.

Dietro le quinte, \texttt{FSDataInputStream}, restituito da
\texttt{FileSystem.open(...)}, utilizza chiamate a procedure remote sul
namenode per ottenere le posizioni dei primi blocchi del file. Per ogni
blocco, il namenode restituisce gli indirizzi dei datanode che lo
contengono, ordinati in base alla prossimità del client. Se il client
stesso è uno dei datanode che contiene un blocco da leggere, il blocco
viene letto localmente.

Alla prima chiamata di \texttt{read()} su \texttt{FSDataInputStream},
l'oggetto si connette al DataNode che contiene il primo blocco del file,
e lo richiede (nell'esempio, \texttt{read} viene chiamato da
\texttt{IOUtils.copyBytes}). Il DataNode risponde inviando i dati
corrispondenti al blocco, fino al termine di questi. Al raggiungimento
della fine di un blocco, \texttt{DFSInputStream} termina la connessione
con il DataNode corrente e ne inizia un'altra con il più prossimo dei
DataNode che contiene il blocco successivo.

In caso di errore dovuto al fallimento di un DataNode o alla ricezione
di un blocco di dati corrotto, il client può ricevere il blocco dal nodo
successivo della lista dei DataNode contenenti il blocco ricevuta dal
NameNode.

I blocchi del file non vengono inviati tutti insieme, e il client deve
periodicamente richiedere al NameNode i dati sui blocchi successivi.
Questo avviene trasparentemente rispetto al client, che si limita a
chiamare \texttt{read} su \texttt{DFSInputStream}.

Le comunicazioni di rete, in questo meccanismo, sono distribuite su
tutto il cluster. Il NameNode riceve richieste che riguardano solo i
metadati dei file, mentre il resto delle connessioni viene eseguito
direttamente tra client e DataNode. Questo approccio permette ad HDFS di
raggiungere un gran livello di scalabilità, evitando i colli di
bottiglia dovuti a un punto di connessione centralizzato nel filesystem.

\section{YARN}\label{yarn}

HDFS è la parte di Hadoop che si occupa di gestire lo storage
distribuito. La computazione distribuita è gestita da YARN, in termini
di gestione delle risorse e di esecuzione del software distribuito.

YARN è un acronimo che sta per Yet Another Resource Negotiator, ed è
l'insieme di API su cui sono implementati framework di programmazione
distribuita di livello più alto, come MapReduce e Spark. YARN si
definisce un \emph{``negotiator''} perché è l'entità che decide quando e
come le risorse del cluster debbano essere allocate per l'esecuzione
distribuita, e che gestisce le comunicazioni che riguardano le risorse
con tutti i nodi coinvolti.

I servizi di YARN sono offerti tramite \emph{demoni} eseguiti nei nodi
del cluster. Ci sono due tipi di demoni in YARN:

\begin{itemize}
\item
  i \textbf{NodeManager}, che utilizzano \emph{container} per eseguire
  con risorse limitate un processo nei nodi del cluster;
\item
  il \textbf{ResourceManager}, di cui ne è eseguita un'istanza per
  cluster, e che ne gestisce le risorse. Il ResourceManager è l'entità
  che comunica con i NodeManager e che decide quali applicazioni i
  container debbano eseguire.
\end{itemize}

I client che vogliono eseguire un'applicazione distribuita in un cluster
Hadoop ne debbono far richiesta al ResourceManager.

\chapter{Batch Processing}\label{batch-processing}

Il Batch Processing è la \emph{raison d'être} di Hadoop. Il primo
paradigma di programmazione per Hadoop, MapReduce, è stato l'unico per
molte release, e ha avuto il grande merito di astrarre la complessità
della computazione batch in ambiente distribuito in funzioni che
associano chiavi e valori a risultati, una grande semplificazione
rispetto ai programmi che gestiscono granularmente l'intricatezza di
ambienti distribuiti.

Pur essendo popolare, MapReduce è soggetto a molte limitazioni, che
riguardano soprattutto la necessità di esprimere i programmi da eseguire
con un modello che non lascia molto spazio alla rielaborazione dei
risultati. Come si vedrà, queste limitazioni sono intrinseche al fatto
che i risultati intermedi vengano salvati nello storage locale del nodo
del cluster, e quelli finali in HDFS. Questi due fattori influenzano
pesantemente le prestazioni che si possono ottenere da un algoritmo,
perché vi introducono l'overhead della lettura e scrittura nel disco, o
peggio in HDFS.

YARN è stato creato proprio per questo motivo: permettere che altri
modelli di computazione diversi da MapReduce potessero essere eseguiti
sfruttando HDFS. Le nuove versioni di MapReduce sono implementate al di
sopra di YARN invece che direttamente in Hadoop, a testimoniare
l'effettiva capacità di YARN di generalizzare i modelli di esecuzione
nei cluster.

La sua alternativa più popolare, Apache Spark, ha API più espressive e
funzionali rispetto a MapReduce, e prestazioni molto più elevate in
molti
algoritmi{[}\protect\hyperlink{ref-mapreduce-spark-performance}{9}{]}.
Tramite astrazioni che offrono un controllo più preciso sul
comportamento dei risultati dell'elaborazione, Spark trova moltissime
applicazioni pratiche sia negli ambiti , tra cui il machine learning

In questa sezione si esaminano MapReduce e Spark, quali sono le
limitazioni di MapReduce che hanno richiesto la necessità di un nuovo
modello computazionale, e quale soluzioni sono offerte da Spark.

\section{MapReduce}\label{mapreduce}

Il modello computazionale di MapReduce è composto, nella sostanza, da
due componenti, che sono intuitivamente il Mapper e il Reducer.

Il Mapper è una classe contenente una funzione \texttt{map}, che riceve
in input una coppia composta da chiave e valore, e che restituisce a sua
volta zero, uno, o più coppie di chiavi e valori\footnote{Dato che Java
  non permette la restituzione di valori multipli da una funzione, per
  ``restituire'' i valori si usa il metodo \texttt{Context.write}
  dell'oggetto \texttt{Context} ricevuto in input da \texttt{map} e
  \texttt{reduce}. È comunque intuitivo pensare ai Mapper e ai Reducer
  come entità che eseguono associazioni da valore a valore.}. Le chiavi
e i valori ricevuti in input dal Mapper sono derivati direttamente
dall'elemento letto in HDFS. Nel caso dei file di testo, ad esempio, la
chiave è un intero che rappresenta la riga del file letto, e il valore è
la riga di testo. È possibile configurare quali chiavi e valori vengano
derivati dalla sorgente e come, creando una classe che implementa
l'interfaccia \texttt{InputMapper} fornita nella libreria di Hadoop.

Le applicazioni MapReduce specificano un proprio Mapper estendendo la
classe \texttt{Mapper} nella libreria di Hadoop, e specificando i tipi
dei parametri generici opportunamente. La firma di \texttt{Mapper} è la
seguente:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{public} \KeywordTok{class}\NormalTok{ Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> }\KeywordTok{extends} \BuiltInTok{Object}
\end{Highlighting}
\end{Shaded}

I tipi di \texttt{KEYIN} e \texttt{VALUEIN} sono gli input della
funzione \texttt{map} del Mapper, e devono corrispondere ai tipi che
l'\texttt{InputFormat} di riferimento restituisce. \texttt{KEYOUT} e
\texttt{VALUEOUT} sono invece i tipi che il Mapper restituisce
rielaborando le chiavi e i valori in input. \texttt{map} ha la seguente
signature:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{protected} \DataTypeTok{void} \FunctionTok{map}\NormalTok{(KEYIN key, VALUEIN value, }\BuiltInTok{Context}\NormalTok{ context) }
    \KeywordTok{throws} \BuiltInTok{IOException}\NormalTok{, }\BuiltInTok{InterruptedException}
\end{Highlighting}
\end{Shaded}

Una volta restituiti dal Mapper, le coppie vengono date in input a una
classe \texttt{Reducer}, che ha una signature simile:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{public} \KeywordTok{class}\NormalTok{ Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>}
\end{Highlighting}
\end{Shaded}

Una classe che estende \texttt{Reducer} ha un metodo \texttt{reduce},
che diversamente dal metodo \texttt{map} riceve in input una chiave, e
un iterabile di tutti i valori che hanno quella stessa chiave:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{protected} \DataTypeTok{void} \FunctionTok{reduce}\NormalTok{(KEYIN key, }\BuiltInTok{Iterable}\NormalTok{<VALUEIN> values, }\BuiltInTok{Context}\NormalTok{ context) }
    \KeywordTok{throws} \BuiltInTok{IOException}\NormalTok{, }\BuiltInTok{InterruptedException}
\end{Highlighting}
\end{Shaded}

Nella fase di reduce, quindi, i valori sono \textbf{aggregati} in base
alla chiave. I valori a questo punto possono essere combinati a seconda
dell'esigenza dell'utente per restituire un risultato finale.

\subsection{Esempio di un programma
MapReduce}\label{esempio-di-un-programma-mapreduce}

Come esempio di programma per MapReduce, si prende in considerazione
l'analisi di log di un web server. Il dataset su cui si esegue
l'elaborazione è fornito liberamente dalla
NASA{[}\protect\hyperlink{ref-nasa-weblog}{10}{]}, e corrisponde ai log
di accesso al server HTTP del NASA Kennedy Space Center dal 1/07/1995 al
31/07/1995. Il log è un file di testo con codifica ASCII, dove ogni riga
corrisponde a una richiesta e ognuna di queste contiene le seguenti
informazioni:

\begin{enumerate}
\tightlist
\item
  L'host che esegue la richiesta, sottoforma di hostname quando
  disponibile o indirizzo IP altrimenti
\item
  Timestamp della richiesta, in formato
  ``\texttt{WEEKDAY\ MONTH\ DAY\ HH:MM:SS\ YYYY}'' e fuso orario, con
  valore fisso \texttt{-0400}.
\item
  La Request-Line HTTP tra virgolette
\item
  Il codice HTTP di risposta
\item
  La dimensione in byte della risposta.
\end{enumerate}

\begin{codelisting}

\caption{Campione di due righe dal log da analizzare}

\hypertarget{lst:log-sample}{\label{lst:log-sample}}
\begin{verbatim}
ntp.almaden.ibm.com - - [24/Jul/1995:12:40:12 -0400] 
    "GET /history/apollo/apollo.html HTTP/1.0" 200 3260

fsd028.osc.state.nc.us - - [24/Jul/1995:12:40:12 -0400]
    "GET /shuttle/missions/missions.html HTTP/1.0" 200 8678
\end{verbatim}

\end{codelisting}

A partire da questo log, si vuole capire quante richieste siano state
ricevute da ogni risorsa HTTP. Un possibile approccio alla risoluzione
del problema è eseguire il parsing di ogni riga del log nel Mapper
utilizzando un'espressione regolare, per estrarre l'URI dalla richiesta.
Il Mapper, per ogni riga, restituische l'URI come chiave e 1 come
valore.

Dopo l'esecuzione dei Mapper, i Reducer riceveranno una coppia formata
dall'URI delle richieste come chiave, e da un iterabile di valori 1, uno
per ogni richiesta. È sufficiente sommare questi valori per ottenere il
numero di richieste finale per l'URI chiave.

\begin{codelisting}

\caption{Implementazione del Mapper utilizzato per analizzare il file di
log.}

\hypertarget{lst:log-mapper}{\label{lst:log-mapper}}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\KeywordTok{import}\ImportTok{ java.io.IOException;}
\KeywordTok{import}\ImportTok{ java.util.regex.Matcher;}
\KeywordTok{import}\ImportTok{ java.util.regex.Pattern;}

\KeywordTok{import}\ImportTok{ org.apache.hadoop.mapreduce.Mapper;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.io.LongWritable;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.io.Text;}

\KeywordTok{public} \KeywordTok{class}\NormalTok{ LogMapper }\KeywordTok{extends}\NormalTok{ Mapper<LongWritable, }\BuiltInTok{Text}\NormalTok{, }\BuiltInTok{Text}\NormalTok{, LongWritable> \{}

    \KeywordTok{private} \DataTypeTok{final} \DataTypeTok{static} \BuiltInTok{Pattern}\NormalTok{ logPattern = }\BuiltInTok{Pattern}\NormalTok{.}\FunctionTok{compile}\NormalTok{(}
        \StringTok{".*}\SpecialCharTok{\textbackslash{}"}\StringTok{[A-Z]+ (.*) HTTP.*"}
\NormalTok{    );}

    \KeywordTok{private} \DataTypeTok{final} \DataTypeTok{static}\NormalTok{ LongWritable one = }\KeywordTok{new} \FunctionTok{LongWritable}\NormalTok{(}\DecValTok{1}\NormalTok{);}

    \AttributeTok{@Override}
    \KeywordTok{protected} \DataTypeTok{void} \FunctionTok{map}\NormalTok{(LongWritable key, }\BuiltInTok{Text}\NormalTok{ value, }\BuiltInTok{Context}\NormalTok{ context)}
            \KeywordTok{throws} \BuiltInTok{IOException}\NormalTok{, }\BuiltInTok{InterruptedException}\NormalTok{ \{}

        \DataTypeTok{final} \BuiltInTok{String}\NormalTok{ request = value.}\FunctionTok{toString}\NormalTok{();}
        \DataTypeTok{final} \BuiltInTok{Matcher}\NormalTok{ requestMatcher = logPattern.}\FunctionTok{matcher}\NormalTok{(request);}

        \KeywordTok{if}\NormalTok{(requestMatcher.}\FunctionTok{matches}\NormalTok{()) \{}
\NormalTok{            context.}\FunctionTok{write}\NormalTok{(}
                \KeywordTok{new} \BuiltInTok{Text}\NormalTok{(requestMatcher.}\FunctionTok{group}\NormalTok{(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                one}
\NormalTok{            );}
\NormalTok{        \}}
\NormalTok{    \}}

\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

Come si può osservere da lst.~\ref{lst:log-mapper}, i tipi utilizzati
dal Mapper non sono tipi standard Java, ma sono forniti dalla libreria.
Hadoop utilizza un suo formato di serializzazione per lo storage e per
la trasmissione dei dati in rete, le cui funzionalità sono accessibili
tramite l'interfaccia \texttt{hadoop.io.Writable}. Le classi
\texttt{LongWritable} e \texttt{Text} sono dei wrapper sui tipi
\texttt{long} e \texttt{String} che forniscono i metodi richiesti
dall'interfaccia di serializzazione, e i valori contenuti in questi tipi
possono essere ottenuti rispettivamente con \texttt{LongWritable.get()}
e \texttt{Text.toString()}.

Per il resto, le operazioni del Mapper sono intuitive: si utilizza
l'espressione regolare per ottenere il token contenente l'URI della
richiesta, e tramite \texttt{context.write} il Mapper invia la coppia
URI e 1.

\begin{codelisting}

\caption{Implementazione del Reducer per il programma di analisi dei
log.}

\hypertarget{lst:log-reducer}{\label{lst:log-reducer}}
\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{import}\ImportTok{ org.apache.hadoop.io.LongWritable;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.io.Text;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.mapreduce.Reducer;}

\KeywordTok{import}\ImportTok{ java.io.IOException;}

\KeywordTok{public} \KeywordTok{class}\NormalTok{ LogReducer }\KeywordTok{extends}\NormalTok{ Reducer<}\BuiltInTok{Text}\NormalTok{, LongWritable, }\BuiltInTok{Text}\NormalTok{, LongWritable> \{}
    \AttributeTok{@Override}
    \KeywordTok{protected} \DataTypeTok{void} \FunctionTok{reduce}\NormalTok{(}\BuiltInTok{Text}\NormalTok{ key, }\BuiltInTok{Iterable}\NormalTok{<LongWritable> values, }\BuiltInTok{Context}\NormalTok{ context)}
            \KeywordTok{throws} \BuiltInTok{IOException}\NormalTok{, }\BuiltInTok{InterruptedException}\NormalTok{ \{}

        \DataTypeTok{long}\NormalTok{ accumulator = }\DecValTok{0}\NormalTok{;}

        \KeywordTok{for}\NormalTok{(LongWritable value: values) \{}
\NormalTok{            accumulator += value.}\FunctionTok{get}\NormalTok{();}
\NormalTok{        \}}

\NormalTok{        context.}\FunctionTok{write}\NormalTok{(key, }\KeywordTok{new} \FunctionTok{LongWritable}\NormalTok{(accumulator));}

\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

Il Reducer, mostrato in lst.~\ref{lst:log-reducer}, prende in input nel
suo metodo \texttt{reduce} i valori aggregati in base alla chiave. Una
volta sommati in una variabile accumulatore, questi vengono scritti in
output in una coppia URI-accumulatore. L'insieme di tutti i valori
restituiti dal Reducer costituiscono l'output finale del programma.

Prima di poter eseguire l'applicazione, è necessario creare un
esecutore, ovvero una classe contenente un punto d'entrata \texttt{main}
che utilizzi le API di Hadoop per eseguire il programma, analogamente a
come descritto in
\protect\hyperlink{esecuzione-di-software-in-hadoop}{Esecuzione di
software in Hadoop}. I lavori MapReduce sono configurati tramite
l'oggetto \texttt{hadoop.mapreduce.Job}, che richiede di specificare le
classi da utilizzare come Mapper e Reducer, assieme ai percorsi dei file
da elaborare. L'esecutore dell'analizzatore di log è mostrato in
lst.~\ref{lst:log-executor}.

\begin{codelisting}

\caption{Esecutore dell'analizzatore di log.}

\hypertarget{lst:log-executor}{\label{lst:log-executor}}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\KeywordTok{import}\ImportTok{ org.apache.hadoop.fs.Path;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.io.LongWritable;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.io.Text;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.mapreduce.Job;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.mapreduce.lib.input.FileInputFormat;}
\KeywordTok{import}\ImportTok{ org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;}

\KeywordTok{public} \KeywordTok{class}\NormalTok{ LogAnalyzer \{}

    \KeywordTok{public} \DataTypeTok{static} \DataTypeTok{void} \FunctionTok{main}\NormalTok{(}\BuiltInTok{String}\NormalTok{ args[]) }\KeywordTok{throws} \BuiltInTok{Exception}\NormalTok{ \{}
        \KeywordTok{if}\NormalTok{(args.}\FunctionTok{length}\NormalTok{ != }\DecValTok{2}\NormalTok{) \{}
            \BuiltInTok{System}\NormalTok{.}\FunctionTok{err}\NormalTok{.}\FunctionTok{println}\NormalTok{(}\StringTok{"Usage: LogAnalyzer <input path> <output path>"}\NormalTok{);}
\NormalTok{        \}}

\NormalTok{        Job job = Job.}\FunctionTok{getInstance}\NormalTok{();}
\NormalTok{        job.}\FunctionTok{setJarByClass}\NormalTok{(LogAnalyzer.}\FunctionTok{class}\NormalTok{);}
\NormalTok{        job.}\FunctionTok{setJobName}\NormalTok{(}\StringTok{"LogAnalyzer"}\NormalTok{);}

\NormalTok{        FileInputFormat.}\FunctionTok{addInputPath}\NormalTok{(job, }\KeywordTok{new} \FunctionTok{Path}\NormalTok{(args[}\DecValTok{0}\NormalTok{]));}
\NormalTok{        FileOutputFormat.}\FunctionTok{setOutputPath}\NormalTok{(job, }\KeywordTok{new} \FunctionTok{Path}\NormalTok{(args[}\DecValTok{1}\NormalTok{]));}

\NormalTok{        job.}\FunctionTok{setMapperClass}\NormalTok{(LogMapper.}\FunctionTok{class}\NormalTok{);}
\NormalTok{        job.}\FunctionTok{setReducerClass}\NormalTok{(LogReducer.}\FunctionTok{class}\NormalTok{);}

\NormalTok{        job.}\FunctionTok{setOutputKeyClass}\NormalTok{(}\BuiltInTok{Text}\NormalTok{.}\FunctionTok{class}\NormalTok{);}
\NormalTok{        job.}\FunctionTok{setOutputValueClass}\NormalTok{(LongWritable.}\FunctionTok{class}\NormalTok{);}

        \BuiltInTok{System}\NormalTok{.}\FunctionTok{exit}\NormalTok{(job.}\FunctionTok{waitForCompletion}\NormalTok{(}\KeywordTok{true}\NormalTok{) ? }\DecValTok{0}\NormalTok{ : }\DecValTok{1}\NormalTok{);}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

L'oggetto \texttt{job} è il centro della configurazione del programma
MapReduce. Tramite questo si specificano il \texttt{jar} contenente le
classi dell'applicazione, il nome del Job, utilizzato per mostrare
descrittivamente nei log e nell'interfaccia web lo stato di
completamente di questo, le classi Mapper e Reducer e i tipi dei valori
di output del Reducer. Vengono impostati anche i path del file di input
e dei file di output, utilizzando i valori ricevuti come parametri in
\texttt{args}. Il job viene effettivamente eseguito alla chiamata di
\texttt{job.waitForCompletion(bool\ verbose)}, che restituisce
\texttt{true} quando questo va a buon fine.

Al termine della compilazione e pacchettizzazione, il programma può
essere eseguito con il comando \texttt{hadoop}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{hadoop}\NormalTok{ LogAnalyzer /example/NASA_access_log_Jul95 /example/LogAnalyzerOutput}

\ExtensionTok{17/07/03}\NormalTok{ 18:17:47 INFO Configuration.deprecation: session.id is deprecated.}
    \ExtensionTok{Instead}\NormalTok{, use dfs.metrics.session-id}
\ExtensionTok{17/07/03}\NormalTok{ 18:17:47 INFO jvm.JvmMetrics: Initializing JVM Metrics with }
    \VariableTok{processName=}\NormalTok{JobTracker, }\VariableTok{sessionId=}
\ExtensionTok{17/07/03}\NormalTok{ 18:17:47 WARN mapreduce.JobResourceUploader: Hadoop command-line }
    \ExtensionTok{option}\NormalTok{ parsing not performed. Implement the Tool interface and execute}
    \ExtensionTok{your}\NormalTok{ application with ToolRunner to remedy this.}
\ExtensionTok{17/07/03}\NormalTok{ 18:17:48 INFO input.FileInputFormat: Total input files to process : 1}
\ExtensionTok{17/07/03}\NormalTok{ 18:17:48 INFO mapreduce.JobSubmitter: number of splits:2}
\ExtensionTok{17/07/03}\NormalTok{ 18:17:48 INFO mapreduce.JobSubmitter: Submitting tokens for }
    \ExtensionTok{job}\NormalTok{: job_local954245035_0001}
\ExtensionTok{17/07/03}\NormalTok{ 18:17:48 INFO mapreduce.Job: The url to track the job: http://localhost:8080/}
\ExtensionTok{17/07/03}\NormalTok{ 18:17:48 INFO mapreduce.Job: Running job: job_local954245035_0001}
\ExtensionTok{17/07/03}\NormalTok{ 18:17:48 INFO mapred.LocalJobRunner: OutputCommitter set in config null}
\ExtensionTok{...}
\end{Highlighting}
\end{Shaded}

Il metodo \texttt{job.waitForCompletion} è stato invocato con il
parametro \texttt{verbose} impostato a \texttt{true}, per cui
l'esecuzione stampa in output un log sul job in esecuzione. È anche
possibile verificare lo stato di esecuzione dei job tramite
un'interfaccia web fornita dal framework.

Al termine dell'esecuzione, i risultati sono disponibili in HDFS nella
cartella \texttt{/example/LogAnalyzerOutput}, come specificato nei
parametri d'esecuzione. I risultati si trovano in una cartella perché
questi sono composti da più file, uno per ogni Reducer eseguito
parallelamente dal framework. In questo caso, il job è stato eseguito da
un solo reducer, per cui i risultati si trovano in un unico file. È
possibile scegliere la quanitità di Reducer da eseguire parallelamente
nel framework, mentre i Mapper, come si vedrà, sono stabiliti in base
all'input.

Eseguendo \texttt{ls} nella cartella di output si può effettivamente
verificare la presenza del file prodotto dal Reducer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{hadoop}\NormalTok{ fs -ls /example/LogAnalyzerOutput}
\ExtensionTok{Found}\NormalTok{ 2 items}
\ExtensionTok{-rw-r--r--}\NormalTok{   3 heygent hdfs          0 2017-07-03 18:17 /example/.../_SUCCESS}
\ExtensionTok{-rw-r--r--}\NormalTok{   3 heygent hdfs     804597 2017-07-03 18:17 /example/.../part-r-0000}
\end{Highlighting}
\end{Shaded}

Assieme al risultato della computazione, MapReduce salva un file vuoto
chiamato \texttt{\_SUCCESS}, di cui si può verificare la presenza in
HDFS per capire se il job è andato a buon fine. Consultando il file, si
può osservare il risultato della computazione eseguita.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{...}
\ExtensionTok{/elv/DELTA/del181.gif}\NormalTok{   71}
\ExtensionTok{/elv/DELTA/del181s.gif}\NormalTok{  390}
\ExtensionTok{/elv/DELTA/deline.gif}\NormalTok{   84}
\ExtensionTok{/elv/DELTA/delseps.jpg}\NormalTok{  90}
\ExtensionTok{/elv/DELTA/delta.gif}\NormalTok{    1492}
\ExtensionTok{/elv/DELTA/delta.htm}\NormalTok{    267}
\ExtensionTok{/elv/DELTA/deprev.htm}\NormalTok{   71}
\ExtensionTok{/elv/DELTA/dsolids.jpg}\NormalTok{  84}
\ExtensionTok{/elv/DELTA/dsolidss.jpg}\NormalTok{ 369}
\ExtensionTok{/elv/DELTA/euve.jpg}\NormalTok{     36}
\ExtensionTok{/elv/DELTA/euves.jpg}\NormalTok{    357}
\ExtensionTok{/elv/DELTA/rosat.jpg}\NormalTok{    38}
\ExtensionTok{/elv/DELTA/rosats.jpg}\NormalTok{   366}
\ExtensionTok{/elv/DELTA/uncons.htm}\NormalTok{   163}
\ExtensionTok{...}
\end{Highlighting}
\end{Shaded}

\subsection{Astrazioni su MapReduce}\label{astrazioni-su-mapreduce}

\section{Spark}\label{spark}

\chapter{Stream Processing}\label{stream-processing}

\section{Kafka}\label{kafka}

\section{Spark Streaming}\label{spark-streaming}

\section{Storm}\label{storm}

\chapter{NoSQL e Big Data}\label{nosql-e-big-data}

\section{Scalabilità e CAP Theorem}\label{scalabilituxe0-e-cap-theorem}

\section{HBase}\label{hbase}

\subsection{Query MapReduce su HBase}\label{query-mapreduce-su-hbase}

\chapter*{Bibliografia}\label{bibliografia}
\addcontentsline{toc}{chapter}{Bibliografia}

\hypertarget{refs}{}
\hypertarget{ref-hadoop-adoption-survey}{}
1. Gartner Survey Highlights Challenges to Hadoop Adoption,
\url{http://www.gartner.com/newsroom/id/3051717}

\hypertarget{ref-hadoop-market-analysis}{}
2. Hadoop Market Forecast 2017-2022,
\url{https://www.marketanalysis.com/?p=279}

\hypertarget{ref-digital-univ}{}
3. The Digital Universe of Opportunities: Rich Data and the Increasing
Value of the Internet of Things,
\url{https://www.emc.com/leadership/digital-universe/2014iview/executive-summary.htm}

\hypertarget{ref-hadoop-doc-main}{}
4. Apache Hadoop Documentation, \url{https://hadoop.apache.org/}

\hypertarget{ref-hadoop-ppa}{}
5. Hadoop Ubuntu Packagers PPA,
\url{https://launchpad.net/~hadoop-ubuntu/+archive/ubuntu/stable}

\hypertarget{ref-hadoop-aur}{}
6. AUR - Hadoop, \url{https://aur.archlinux.org/packages/hadoop/}

\hypertarget{ref-mapr-fs}{}
7. MapR-FS Overview, \url{https://mapr.com/products/mapr-fs/}

\hypertarget{ref-hadoop-guide-hdfs-file-read}{}
8. White, T.: Hadoop: The Definitive Guide. (2015)

\hypertarget{ref-mapreduce-spark-performance}{}
9. Shi, J., Qiu, Y., Farooq Minhas, U., Jiao, L., Wang, C., Reinwald,
B., Ozcan, F.: Clash of the Titans: MapReduce vs.~Spark for Large Scale
Data Analytics, \url{http://www.vldb.org/pvldb/vol8/p2110-shi.pdf}

\hypertarget{ref-nasa-weblog}{}
10. HTTP Logs from the KSC-NASA,
\url{http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html}

\end{document}
