\documentclass[italian,a4paper, twoside, 12pt]{report}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Palatino Linotype}
    \setmonofont[Mapping=tex-ansi]{InconsolataForPowerline Nerd Font}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage{fancyvrb}
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=NavyBlue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\VerbatimFootnotes % allows verbatim text in footnotes
\usepackage[a4paper, left=3.5cm, right=3cm, top=2.5cm, bottom=3cm]{geometry}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=italian]{babel}
\else
  \usepackage{polyglossia}
  \setmainlanguage[]{italian}
\fi
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage[dvipsnames]{xcolor}
\usepackage{textcomp}
\usepackage{tikz}
\usetikzlibrary{calc}
\addto\captionsitalian{\renewcommand{\chaptername}{Parte}}
\renewcommand{\baselinestretch}{1.1} 
% \renewcommand\thechapter{\Roman{chapter}}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\usepackage{listings}

%\usepackage{subfig}
%\usepackage{float}
%\floatstyle{plain}
%\makeatletter
%\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
%\makeatother
%\floatname{codelisting}{Listato}
%\newcommand*\listoflistings{\listof{codelisting}{Elenco dei Listati}}

\date{}

\begin{document}

\definecolor{unicamblue}{HTML}{002860}

\newgeometry{left=2.25cm,right=2.25cm,top=1cm,bottom=1cm,a4paper}

\begin{titlepage}
    \color{unicamblue}

\begin{tikzpicture}[remember picture, overlay]
    \color{black}
  \draw[line width = 0.5pt] ($(current page.north west) + (1cm,-1cm)$) rectangle ($(current page.south east) + (-1cm,1cm)$);
\end{tikzpicture}

\begin{center}

\vspace{1cm}
\Huge\sc Università degli Studi di Camerino\\
\vspace{3mm}
\huge\bf Scuola di Scienze e Tecnologie\\
\vspace{3mm}
    \LARGE \bf \textsl{Corso di Laurea in Informatica}
\end{center}

\vspace{14mm}

\begin{center}

\includegraphics[width=3.5cm]{img/unicam_logo.pdf}\\

\vspace{1.5cm}
{\Huge{\bf Big Data}}\\
\vspace{5mm}
{\huge{\bf Tecniche e tool di analisi}}\\
\vspace{15mm} {\huge{Elaborato Finale}}

\end{center}

\vspace{25mm}
\par
\noindent

\begin{minipage}[t]{0.47\textwidth}
    \begin{center}
    {\LARGE{\textsl{Laureando}
    \vspace{0.2cm}\\
    \textbf{Emanuele Gentiletti}\\
    \vspace{1.7cm}
    Matricola: \textbf{090150}
    }}

    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\textwidth}
    \begin{center}
    {\LARGE{\textsl{Relatore}
    \vspace{0.2cm}\\
    \bf Prof. Diletta Romana Cacciagrano\\
    }}

    \end{center}
\end{minipage}
\vspace{35mm}
\begin{center}
\rule[0.1cm]{14cm}{0.1mm}\\
\vspace{2mm}
{\Large{\textsl{Anno Accademico 2016/2017}}}
\end{center}
\end{titlepage}
\restoregeometry
\clearpage
\cleardoublepage

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{2}
\tableofcontents
}
\clearpage

\chapter*{Introduzione}\label{introduzione}
\addcontentsline{toc}{chapter}{Introduzione}

Negli ultimi decenni, i Big Data hanno preso piede in modo impetuoso in
una grande varietà di ambiti. Settori come medicina, finanza, business
analytics e marketing sfruttano i Big Data per guidare lo sviluppo,
utilizzando tecnologie che riescono a ricavare valore da grandi dataset
in tempi eccezionalmente brevi rispetto al passato.

L'innovazione che rende possibili questi risultati è stata guidata dal
software molto più che dall'hardware. Sono stati i cambiamenti nel modo
di pensare alla computazione e all'organizzazione dei suoi processi che
hanno portato a risultati notevoli nell'efficienza di elaborazione di
grandi quantità di dati.

Uno dei fattori più importanti ad aver dato slancio a questo fenomeno è
stato lo sviluppo di Hadoop, un framework open source progettato per la
computazione batch di dataset di grandi dimensioni. Utilizzando
un'architettura ben congeniata, Hadoop ha permesso l'analisi in tempi
molto rapidi di interi dataset di dimensioni nell'ordine dei terabyte,
fornendo una capacità di sfruttamento di questi, e conseguentemente un
valore molto più alti.

Una delle conseguenze più importanti di Hadoop è stata una
democratizzazione delle capacità di analisi dei dati:

\begin{itemize}
\tightlist
\item
  Hadoop è sotto licenza Apache, permettendo a chiunque di utilizzarlo a
  scopi commerciali e non;
\item
  Hadoop non richiede hardware costoso ad alta affidabilità, e
  incoraggia l'adozione di macchine più generiche e prone al fallimento
  per il suo uso, che possono essere ottenute a costi inferiori;
\item
  Il design di Hadoop permette la sua esecuzione in cluster di macchine
  eterogenee nel software e nell'hardware che possono essere acquisite
  da diversi rivenditori, un altro fattore che permette l'abbattimento
  dei costi;
\item
  I vari modelli di programmazione in Hadoop hanno in comune
  l'astrazione della computazione distribuita e dei problemi intricati
  che questa comporta, abbassando la barriere in entrata in termini di
  conoscenze e lavoro richiesti per creare programmi che necessitano di
  un altro grado di parallelismo.
\end{itemize}

Questi fattori hanno spinto a una vasta adozione di Hadoop e
dell'ecosistema software che lo circonda, in ambito aziendale e
scientifico. L'adozione di Hadoop, secondo un sondaggio fatto a maggio
2015{[}@hadoop-adoption-survey{]}, si aggira al 26\% delle imprese negli
Stati Uniti, e si prevede che il mercato attorno ad Hadoop sorpasserà i
16 miliardi di dollari nel 2020 {[}@hadoop-market-analysis{]}.

Tutto questo accade in un'ottica in cui la produzione di informazioni
aumenta ad una scala senza precedenti: secondo uno studio di
IDC{[}@digital-univ{]}, la quantità di informazioni nell'``Universo
Digitale'' ammontava a 4.4 TB nel 2014, e la sua dimensione stimata nel
2020 è di 44 TB. Data la presenza di questa vasta quantità di
informazioni, il loro sfruttamento efficace può essere fonte di grandi
opportunità.

In questo documento si analizzano le varie tecniche che sono a
disposizione per l'utilizzo effettivo dei Big Data, come queste
differiscono tra di loro, e quali strumenti le mettono a disposizione.
Si parlerà inoltre di come gli strumenti possano essere integrati in
sistemi di produzione esistenti, le possibili architetture di un sistema
di questo tipo e come \ldots{}

La gestione di sistemi per l'elaborazione di Big Data richiede una
configurazione accurata per ottenere affidabilità e fault-tolerance. Pur
sottolineando che l'importanza di questi aspetti non è da sottovalutare,
questa tesi si concentrerà più sul modello computazionale e di
programmazione che gli strumenti offrono.

\chapter{Big Data e Paradigmi di
Elaborazione}\label{big-data-e-paradigmi-di-elaborazione}

Per Big Data si intendono collezioni di dati con caratteristiche tali da
richiedere strumenti innovativi per poterli gestire e analizzare. Uno
dei modelli tradizionali e più popolari per descrivere le
caratteristiche dei Big Data si chiama \textbf{modello delle 3V}. Il
modello identifica i Big Data come collezioni di informazione che
presentano grande abbondanza in una o più delle seguenti
caratteristiche:

\begin{itemize}
\tightlist
\item
  Il \textbf{volume} delle informazioni, che può aggirarsi dalle decine
  di terabyte per arrivare fino ai petabyte;
\item
  La \textbf{varietà}, intesa come la varietà di \emph{fonti} e di
  \emph{possibili strutturazioni} delle informazioni di interesse;
\item
  La \textbf{velocità} di produzione delle informazioni di interesse.
\end{itemize}

Ognuno dei punti di questo modello deriva da esigenze che vanno ad
accentuarsi andando avanti nel tempo, in particolare:

\begin{itemize}
\item
  Il volume delle collezioni dei dati è aumentato esponenzialmente in
  tempi recenti, con l'avvento dei Social Media, dell'IOT, e degli
  smartphone. Generalizzando, i fattori che hanno portato a un grande
  incremento del volume dei data set sono un aumento della generazione
  automatica di dati da parte dei dispositivi e dei contenuti prodotti
  dagli utenti.
\item
  L'aumento dei dispositivi e dei dati generati dagli utenti portano
  conseguentemente a un aumento delle fonti, gestite da enti e persone
  diverse. Per questa ragione, le strutture dei dati ricavati
  difficilmente saranno uniformi. Inoltre, l'utilizzo di dati non
  strutturati rigidamente è prevalente nelle tecnologie consumer,
  business e scientifiche (come documenti JSON, XML e CSV), che sono
  spesso un obiettivo auspicabile per l'analisi.
\item
  Si possono fare le stesse considerazioni fatte per il volume dei dati
  per quanto riguarda la velocità. I flussi di dati vengono generati dai
  dispositivi e dagli utenti, che li producono a ritmi molto più
  incalzanti rispetto agli operatori.
\end{itemize}

Per l'elaborazione di dataset con queste caratteristiche sono stati
sviluppati molti strumenti, che usano diversi pattern di elaborazione a
seconda delle esigenze dell'utente e del tipo di dati con cui si ha a
che fare. I modelli di elaborazione più importanti e rappresentativi
sono il \emph{batch processing} e lo \emph{stream processing}.

\section{Batch e Streaming
Processing}\label{batch-e-streaming-processing}

Il batch processing è il pattern di elaborazione generalmente più
efficiente, e consiste nell'elaborare un intero dataset in un'unità di
lavoro, per poi ottenere i risultati al termine di questa.

Questo approccio è ottimale quando i dati da elaborare sono disponibili
a priori, e non c'è necessità di ottenere i risultati in tempi immediati
o con bassa latenza. Tuttavia, questo approccio ha dei limiti.

\begin{itemize}
\item
  Le fasi del batch processing richiedono la schedulazione dei lavori da
  parte dell'utente, con un conseguente overhead dovuto alla
  schedulazione in sé o alla configurazione di strumenti automatizzati
  che se ne occupino;
\item
  Non è possibile accedere ai risultati prima del termine del job, che
  può avere una durata eccessiva rispetto alle esigenze
  dell'applicazione o dell'utente.
\end{itemize}

Per use case in cui questi fattori sono rilevanti, lo \textbf{stream
processing} si presta come più adatto. In questo paradigma, i dati da
elaborare vengono ricevuti da \emph{stream}, che rappresentano flussi di
dati contigui provenienti da origini non necessariamente controllate.
Gli stream forniscono nuovi dati in modo \emph{asincrono}, e la loro
elaborazione avviene a ogni nuovo evento di ricezione. I job in
streaming molto spesso non hanno un termine prestabilito, ma vengono
terminati dall'utente, e i risultati dell'elaborazione possono essere
disponibili mano a mano che l'elaborazione procede, permettendo quindi
un feedback più rapido rispetto ai lavori batch.

\subsection{\texorpdfstring{\emph{Data at Rest} e \emph{Data in
Motion}}{Data at Rest e Data in Motion}}\label{data-at-rest-e-data-in-motion}

I due paradigmi si differenziano anche per il modo in cui i dati sono
disponibili. Il processing batch richiede che l'informazione sia
\emph{data at rest}, ovvero informazioni completamente accessibili a
priori dal programma. I dati di input in una computazione batch sono
determinati al suo inizio, e non possono cambiare durante il suo corso.
Questo significa che se si rende desiderabile dare in input una nuova
informazione in un lavoro batch, l'unico modo per farlo è rieseguire
interamente il lavoro.

Lo \textbf{stream processing}, invece, è progettato per \emph{data in
motion}, dati in arrivo continuo non necessariamente disponibili prima
dell'inizio dell'elaborazione. Esempi di data in motion possono essere
rappresentati dai dati ricevuti in un socket TCP inviati da reti di
sensori IOT, o dall'ascolto di servizi di social media.

È possibile utilizzare strumenti di processing in streaming anche per
\emph{data at rest}, rappresentando il dataset come uno stream. Questa
proprietà è desiderabile, perché permette di utilizzare le stesse
applicazioni per elaborazioni che riguardano dataset disponibili a
priori e stream di cui non si ha completo controllo.

\begin{longtable}[]{@{}lll@{}}
\caption{Differenze tra elaborazione batch e streaming}\tabularnewline
\toprule
\begin{minipage}[b]{0.27\columnwidth}\raggedright\strut
Caratteristiche\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
Batch\strut
\end{minipage} & \begin{minipage}[b]{0.43\columnwidth}\raggedright\strut
Streaming\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.27\columnwidth}\raggedright\strut
Caratteristiche\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
Batch\strut
\end{minipage} & \begin{minipage}[b]{0.43\columnwidth}\raggedright\strut
Streaming\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Ottimizzazione\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
Alto throughput\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright\strut
Bassa latenza\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Tipo di informazione\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\emph{Data at rest}\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright\strut
\emph{Data in motion} e \emph{Data at rest}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Accesso ai dati\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
Stabilito all'inizio\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright\strut
Dipendente dallo stream\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Accesso ai risultati\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
Fine job\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright\strut
Continuo\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Un esempio di \emph{data at rest} sono i resoconti delle vendite di
un'azienda, su cui si possono cercare pattern per identificare quali
prodotti sono in trend nelle vendite. Per \emph{data in motion} si può
considerare l'invio di dati da parte di sensori IoT o le pubblicazioni
degli utenti nei social media, che sono continui e senza una fine
determinata.

\subsection{Architetture di sistemi Big
Data}\label{architetture-di-sistemi-big-data}

I paradigmi di Batch e Stream processing presentano differenze notevoli
nelle astrazioni, nei tool e nelle API utilizzate. Il loro utilizzo è
condizionat

Ad oggi, le architetture dei sistemi che sfruttano i Big Data si basano
principalmente su due modelli, la \textbf{lambda} e la \textbf{kappa}
architecture.

\begin{figure}
\centering
\includegraphics[width=0.75000\textwidth]{img/lambda_architecture.png}
\caption{Diagramma della Lambda Architecture}
\end{figure}

La lambda architecture utilizza tre unità logiche, il \textbf{batch
layer}, lo \textbf{speed layer} e il \textbf{serving layer}. Il serving
layer è un servizio o un insieme di servizi che permettono di eseguire
query sui dati elaborati dagli altri due layer. Il batch layer esegue
framework per computazioni batch, mentre lo speed layer esegue
computazioni stream. Questi due layer rendono disponibili i risultati
delle loro computazioni al serving layer per la consultazione da parte
degli utenti.

Il \textbf{batch layer} opera sui dati archiviati storicamente, e
riesegue le computazioni periodicamente per integrare i nuovi dati
ricevuti. Questo layer può eseguire velocemente computazioni sulla
totalità dei dati. Lo \textbf{speed layer} invece elabora i dati
asincronamente alla loro ricezione, e offre risultati con una bassa
latenza.

Questo approccio è il più versatile, perché permette l'utilizzo di
entrambi i paradigmi e della totalità degli strumenti progettati per
batch e stream processing. Tuttavia, i layer batch e speed richiedono
una gestione separata, e il mantenimento di due basi di codice scritte
con API e potenzialmente linguaggi diversi, anche per applicazioni che
eseguono le stesse funzioni. I sistemi che implementano architetture
lambda sono i più onerosi nello sviluppo e nella manutenzione.

\begin{figure}
\centering
\includegraphics[width=0.75000\textwidth]{img/kappa_architecture.png}
\caption{Diagramma della Kappa Architecture}
\end{figure}

In contrapposizione, la kappa architecture non utilizza un batch layer,
e la totalità delle computazioni viene eseguita dallo speed layer. Per
eseguire elaborazioni sui dati archiviati, questi vengono rappresentati
come uno stream, che viene dato in ingestione allo speed layer. In
questo modo gli strumenti e le basi di codice possono essere unificate,
semplificando l'architettura e rendendo la gestione del sistema meno
impegnativa.

Le differenze tra i due approcci sono più visibili quando si mettono a
confronto i framework di elaborazione batch e streaming per osservare le
differenze nell'uso. Come regola generale, si può definire preferibile
la lambda architecture per l'efficienza delle computazioni, superiore
nei sistemi di elaborazione batch. La lambda architecture è preferibile
quando le elaborazioni che si vogliono eseguire sui dati storici e
quelli in arrivo sono identiche o molto simili, o si vuole ottenere un
sistema architetturalmente più semplice. Spesso la scelta dipende da un
tradeoff tra questi due fattori.

\hypertarget{hadoop}{\chapter{Hadoop}\label{hadoop}}

Hadoop è una piattaforma software utilizzata per lo storage e la
computazione distribuita di dataset di grandi dimensioni. Hadoop viene
eseguito in \emph{cluster} di computer, che vengono coordinati dalla
piattaforma per fornire delle API in grado di astrarre una parte
importante della complessità insita nei sistemi distribuiti. Hadoop
fornisce delle interfacce per l'elaborazione diretta dei dati da parte
degli utenti, e delle primitive di livello più basso che consentono
l'implementazione di altri framework basati sulla sua infrastruttura di
base. Grazie a quest'ultima caratteristica, Hadoop è diventato un perno
centrale nell'ambito dei Big Data, su cui si è costruito un ecosistema
di tool e tecnologie che ne utilizzano l'infrastruttura o con cui viene
fornita una stretta integrazione.

La documentazione ufficiale{[}@hadoop-doc-main{]} lo descrive come:

\begin{quote}
\ldots{}un framework che abilita l'elaborazione distribuita di grandi
dataset in cluster di computer utilizzando semplici modelli di
programmazione. \protect\hyperlink{hadoop}{Hadoop} è progettato per
essere scalato da server singoli a migliaia di macchine, dove ognuna di
queste offre computazione e storage locale. Invece di affidarsi
all'hardware per fornire un'alta affidabilità,
\protect\hyperlink{hadoop}{Hadoop} è progettato per rilevare e gestire i
fallimenti {[}delle computazioni{]} a livello applicativo, mettendo a
disposizione un servizio ad alta affidiabilità su cluster di computer
proni al fallimento.
\end{quote}

In questa definizione sono racchiusi dei punti importanti:

\begin{itemize}
\item
  \textbf{Semplici modelli di programmazione}

  Hadoop raggiunge molti dei suoi obiettivi fornendo un'interfaccia di
  alto livello al programmatore, in modo di potersi assumere la
  responsabilità di molti concetti complessi e necessari alla
  correttezza e all'efficienza della computazione distribuita, ma che
  hanno poco a che fare con il problema da risolvere in sé (come la
  sincronizzazione di task paralleli e lo scambio dei dati tra nodi del
  sistema distribuito).

  Il framwork fornisce un modello di programmazione distribuita rivolto
  agli utenti, chiamato MapReduce, e ne esistono molti altri creati da
  terze parti.
\item
  \textbf{Computazione e storage locale}

  L'ottimizzazione più importante che Hadoop fornisce rispetto
  all'elaborazione dei dati è il risultato dell'unione di due concetti:
  \textbf{distribuzione dello storage} e \textbf{distribuzione della
  computazione}.

  Entrambi sono importanti a prescindere dell'uso particolare che ne fa
  Hadoop: la distribuzione dello storage permette di combinare lo spazio
  fornito da più dispositivi e di farne uso tramite un'unica interfaccia
  logica, e di replicare i dati in modo da poter tollerare guasti nei
  dispositivi. La distribuzione della computazione permette di aumentare
  il grado di parallelizazione nell'esecuzione dei programmi.

  Hadoop unisce i due concetti utilizzando cluster di macchine che hanno
  sia lo scopo di mantenere lo storage, che quello di elaborare i dati.
  Quando Hadoop esegue un lavoro, \textbf{quante più possibili delle
  computazioni richieste vengono eseguite nei nodi che già contengono i
  dati da elaborare}. Questo permette di ridurre la latenza di rete,
  minimizzando la quantità di dati che devono essere scambiati tra i
  nodi del cluster. Il meccanismo è trasparente all'utente, a cui basta
  persitere i dati da elaborare nel cluster e utilizzare un framework
  basato su Hadoop per usifruirne. Questo principio viene definito
  \textbf{data locality}.
\item
  \textbf{Rack awareness}

  Nel contesto di Hadoop, \emph{rack awareness} si riferisce a delle
  ottimizzazioni sull'utilizzo di banda di rete e sull'affidabilità che
  Hadoop fa basandosi sulla struttura del cluster.

  \begin{figure}
  \def\svgwidth{\linewidth}
  \input{img/hadoop_topology.pdf_tex}
  \label{fig:hadoop-topology}
  \caption{Topologia di rete tipica di un cluster Hadoop.}
  \end{figure}

  Quando configurato per essere \emph{rack aware}, Hadoop considera il
  cluster come un insieme di \emph{rack} che contengono i nodi del
  cluster. Tutti i nodi di un rack sono connessi a uno switch di rete (o
  dispositivo equivalente), e tutti gli switch sono a loro volta
  connessi a uno switch centrale.

  A partire da questa struttura si può fare un'assunzione importante: la
  comunicazione tra nodi in uno stesso rack è meno onerosa in termini di
  banda rispetto alla comunicazione tra nodi in rack diversi, perché la
  comunicazione può essere commutata tramite un solo switch.

  Quando possibile, Hadoop utilizza questo principio per minimizzare
  l'uso di banda tra nodi del cluster. Come si vedrà, i vari componenti
  di Hadoop fanno uso della configurazione di rete per ottimizzazare
  dell'uso della rete e per ottenere una migliore fault-tolerance.
\item
  \textbf{Scalabilità}

  Hadoop è in grado di scalare linearmente in termini di velocità di
  computazione e storage, ed è in grado di sostenere cluster composti da
  un gran numero di macchine. Il più grande cluster Hadoop conosciuto
  dal punto di vista dello storage è gestito da Facebook, che secondo
  gli ultimi dati disponibili nell'anno 2011 conteneva 21 petabyte di
  dati ed è composto da più di 2000 nodi.\textbar{}\textbar{}\textbar{}
\item
  \textbf{Hardware non necessariamente affidabile}

  I cluster di macchine che eseguono Hadoop non hanno particolari
  requisiti di affidabilità. Il framework è progettato per tenere in
  conto dell'alta probabilità di fallimento dell'hardware, e per
  attenuarne le conseguenze, sia dal punto di vista dello storage e
  della potenziale perdita di dati, che da quello della perdita di
  risultati intermedi e parziali nel corso dell'esecuzione di lavori
  computazionalmente costosi. In questo modo l'utente è sgravato dal
  compito generalmente difficile di gestire fallimenti parziali nel
  corso delle computazioni.
\end{itemize}

Hadoop è composto da diversi moduli:

\begin{itemize}
\item
  \textbf{HDFS}, un filesystem distribuito ad alta affidabilità, che
  fornisce replicazione automatica all'interno dei cluster e accesso ad
  alto throughput ai dati
\item
  \textbf{YARN}, un framework per la schedulazione di lavori e per la
  gestione delle risorse all'interno del cluster
\item
  \textbf{MapReduce}, un framework e un modello di programmazione
  fornito da Hadoop per la scrittura di programmi paralleli che
  processano grandi dataset.
\end{itemize}

\hypertarget{installazione-e-configurazione}{\section{Installazione e
Configurazione}\label{installazione-e-configurazione}}

Ogni versione di Hadoop viene distribuita in tarball, una con i
sorgenti, da cui si può eseguire una build manuale, e una binaria. Per
un approccio più strutturato, sono disponibili repository che forniscono
versioni pacchettizzate di Hadoop, come il PPA per
Ubuntu{[}@hadoop-ppa{]} e i pacchetti AUR per Arch
Linux{[}@hadoop-aur{]}.

Ci sono anche distribuzioni di immagini virtuali Linux create
appositamente con lo scopo di fornire un ambiente preconfigurato di
prototipazione con Hadoop e vari componenti del suo ecosistema. I due
ambienti più utilizzati di questo tipo sono Cloudera QuickStart e
HortonWorks Sandbox, disponibili per VirtualBox, VMWare e Docker. Gli
esempi di questo documento sono eseguiti prevalentemente da Arch Linux e
dalla versione Docker di HortonWorks Sandbox.

Hadoop è configurabile tramite file XML, che si trovano rispetto alla
cartella d'installazione in \lstinline!etc/hadoop!. Ogni componente di
Hadoop (HDFS, MapReduce, Yarn) ha un file di configurazione apposito che
contiene le sue impostazioni, e un file di configurazione globale per il
cluster contiene proprietà comuni a tutti i componenti.

\begin{longtable}[]{@{}llll@{}}
\caption{Nomi dei file di configurazione per i componenti di
Hadoop}\tabularnewline
\toprule
\begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
Comuni\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
HDFS\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
YARN\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\raggedright\strut
MapReduce\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
Comuni\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
HDFS\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
YARN\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\raggedright\strut
MapReduce\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.22\columnwidth}\raggedright\strut
\lstinline!core-site.xml!\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright\strut
\lstinline!hdfs-site.xml!\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright\strut
\lstinline!yarn-site.xml!\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright\strut
\lstinline!mapred-site.xml!\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{codelisting}

\caption{Esempio di file di configurazione personalizzato di Hadoop.}

\begin{lstlisting}[language=XML, label=lst:hadoop-conf-example]
<?xml version="1.0"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode/</value>
    </property>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>resourcemanager:8032</value>
    </property>
</configuration>
\end{lstlisting}

\end{codelisting}

È possibile selezionare un'altra cartella da cui prendere i file di
configurazione, impostandola come valore della variabile d'ambiente
\lstinline!HADOOP_CONF_DIR!. Un approccio comune alla modifica dei file
di configurazione consiste nel copiare il contenuto di
\lstinline!etc/hadoop! in un'altra posizione, specificare questa in
\lstinline!HADOOP_CONF_DIR! e fare le modifiche nella nuova cartella. In
questo modo si evita di modificare l'albero d'installazione di Hadoop.

Per molti degli eseguibili inclusi in Hadoop, è anche possibile
specificare un file che contiene ulteriori opzioni di configurazione,
che possono sovrascrivere quelle in \lstinline!HADOOP_CONF_DIR! tramite
lo switch \lstinline!-conf!.

\hypertarget{esecuzione-di-software-in-hadoop}{\subsection{Esecuzione di
software in Hadoop}\label{esecuzione-di-software-in-hadoop}}

I programmi che sfruttano il runtime di Hadoop sono generalmente
sviluppati in Java (o in un linguaggio che ha come target di
compilazione la JVM), e vengono avviati tramite l'eseguibile
\lstinline!hadoop!. L'eseguibile richiede che siano specificati il
classpath del programma, e una classe contente un metodo
\lstinline!main! che si desidera eseguire (analogo all'entry point dei
programmi Java).

Il classpath può essere specificato tramite la variabile d'ambiente
\lstinline!HADOOP_CLASSPATH!, che può essere il percorso di una
directory o di un file \lstinline!jar!. La classe con il metodo
\lstinline!main! da invocare viene messa tra i parametri del comando
\lstinline!hadoop!, seguita dagli argomenti che si vogliono passare in
\lstinline!args[]!.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Volendo eseguire il seguente programma in Hadoop:

\begin{lstlisting}[language=Java]
public class SayHello {
    public static void main(String args[]) {
        System.out.println("Hello " + args[0] + "!");
    }
}
\end{lstlisting}

  Lo si può compilare e pacchettizzare in un file \lstinline!jar!, per
  poi utilizzare i seguenti comandi:

\begin{lstlisting}[language=sh]

$~ export HADOOP_CLASSPATH=say_hello.jar 
$~ hadoop SayHello Josh

Hello Josh!
\end{lstlisting}
\item
  In alternativa, si può eseguire il comando \lstinline!hadoop jar!, e
  specificare il file \lstinline!jar! direttamente nei suoi argomenti:

\begin{lstlisting}[language=sh]

$~ hadoop jar say_hello.js SayHello Josh

Hello Josh!
\end{lstlisting}
\end{enumerate}

In generale, i programmi eseguiti in Hadoop fanno uso della sua libreria
client. La libreria fornisce accesso al package
\lstinline!org.apache.hadoop!, che contiene le API necessarie per
interagire con Hadoop. Non è necessario che la libreria client si trovi
nel classpath finale, in quanto il runtime di Hadoop fornisce le classi
della libreria a runtime.

Per gestire le dipendenze e la pacchettizzazione dei programmi per
Hadoop è pratico utilizzare un tool di gestione delle build. Negli
esempi in questo documento si utilizza Maven a questo scopo, che
permette di specificare le proprietà di un progetto, tra cui le sue
dipendenze, in un file XML chiamato POM (Project Object Model). A
partire dal POM, Maven è in grado di scaricare automaticamente le
dipendenze del progetto, e di pacchettizzarle correttamente negli
artefatti \lstinline!jar! a seconda della configurazione fornita.

\begin{codelisting}

\caption{Un esempio semplificato di un POM per il programma SayHello.}

\begin{lstlisting}[language=XML, numbers=left, label=lst:pom_example]
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="..." >
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.example</groupId>
    <artifactId>say_hello</artifactId>
    <version>1.0</version>

    <dependencies>

        <!-- Libreria client di Hadoop -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>2.8.1</version>
            <scope>provided</scope>
        </dependency>

    </dependencies>
</project>
\end{lstlisting}

\end{codelisting}

Maven è in grado di gestire correttamente la dipendenza della libreria
client di Hadoop, attraverso un meccanismo chiamato \emph{dependency
scope}. Per ogni dipendenza è possibile specificare una proprietà
\emph{scope}, che indica in che modo la dipendenza debba essere gestita
a tempo di build (in particolar modo, se debba essere inclusa nel
classpath). Se non specificato, lo scope è impostato a
\lstinline!compile!, che indica che la dipendenza è resa disponibile nel
classpath dell'artefatto. Per gestire correttamente la dipendenza dalla
libreria client di Hadoop, è opportuno impostare lo scope della
dipendenza a \lstinline!provided!, che indica che le classi della
libreria sono fornite dal container in cui è eseguito il programma.

\section{HDFS}\label{hdfs}

HDFS è un filesystem distribuito che permette l'accesso ad alto
throughput ai dati, scritto in Java ed eseguito nello userspace. HDFS è
stato studiato e progettato per fornire un sistema di storage
distribuito che permetta l'efficiente elaborazione batch di grandi
dataset, e che sia resiliente al fallimento delle singole macchine del
cluster.

I dati contenuti in HDFS sono organizzati, a livello di storage, in
unità logiche chiamate \emph{blocchi}, nel senso comune del termine nel
dominio dei filesystem. I blocchi di un singolo file possono essere
distribuiti all'interno di più macchine all'interno del cluster,
permettendo di avere file più grandi della capacità di storage di ogni
singola macchina nel cluster. Rispetto ai filesystem comuni la
dimensione di un blocco è molto più grande, 128 MB di default. La
ragione per cui HDFS utilizza blocchi così grandi è minimizzare il costo
delle operazioni di seek, dato il fatto che se i file sono composti da
meno blocchi, si rende necessario trovare l'inizio di un blocco un minor
numero di volte. Questo approccio riduce anche la frammentazione dei
dati, rendendo più probabile che questi vengano scritti contiguamente
all'interno della macchina\footnote{Non è possibile essere certi della
  contiguità dei dati, perché HDFS non è un'astrazione diretta sulla
  scrittura del disco, ma sul filesystem del sistema operativo che lo
  esegue. La frammentazione effettiva dipende da come i dati vengono
  organizzati dal filesystem del sistema operativo.}.

HDFS è basato sulla specifica POSIX, e ha quindi una struttura
gerarchica. L'utente può strutturare i dati salvati in directory, e
impostare permessi di accesso in file e cartelle. Tuttavia, l'adesione a
POSIX non è rigida, e alcune operazioni non sono rese possibili, come la
modifica dei file in punti arbitrari. Queste restrizioni permettono ad
HDFS di implementare efficientemente funzioni specifiche del suo dominio
(come il batch processing), e di semplificare la sua architettura.

\subsection{Principi architetturali}\label{principi-architetturali}

\begin{figure}
\centering
\includegraphics{img/hdfsarchitecture.png}
\caption{Schema di funzionamento dell'architettura di HDFS}
\end{figure}

La documentazione di Hadoop descrive i seguenti come i principi
architetturali alla base della progettazione di HDFS:

\begin{itemize}
\item
  \textbf{Fallimento hardware come regola invece che come eccezione}

  Un sistema che esegue HDFS è composto da molti componenti, con
  probabilità di fallimento non triviale. Sulla base di questo
  principio, HDFS da' per scontato che \textbf{ci sia sempre un numero
  di componenti non funzionanti}, e si pone di rilevare errori e guasti
  e di fornire un recupero rapido e automatico da questi.

  Il meccanismo principale con cui HDFS raggiunge questo obiettivo è la
  replicazione: in un cluster, ogni blocco di cui un file è composto è
  replicato in più macchine (3 di default). Se un blocco non è
  disponibile in una macchina, o se non supera i controlli di integrità,
  una sua copia può essere letta da un'altra macchina in modo
  trasparente per il client.

  Il numero di repliche per ogni blocco è configurabile, e ci sono più
  criteri con cui viene deciso in quali macchine il blocco viene
  replicato, principalmente orientati al risparmio di banda di rete.
\item
  \textbf{Modello di coerenza semplice}

  Per semplificare l'architettura generale, HDFS fa delle assunzioni
  specifiche sul tipo di dati che vengono salvati in HDFS e pone dei
  limiti su come l'utente possa lavorare sui file. In particolare,
  \textbf{non è possibile modificare arbitrariamente file già
  esistenti}, e le modifiche devono limitarsi a operazioni di
  troncamento e di aggiunta a fine file. Queste supposizioni permettono
  di semplificare il modello di coerenza, perché i blocchi di dati, una
  volta scritti, possono essere considerati immutabili, evitando una
  considerevole quantità di problemi in un ambiente dove i blocchi di
  dati sono replicati in più posti:

  \begin{itemize}
  \item
    Per ogni modifica a un blocco di dati, bisognerebbe verificare quali
    altre macchine contengono il blocco, e rieseguire la modifica (o
    rireplicare il blocco modificato) in ognuna di queste.
  \item
    Queste modifiche dovrebbero essere fatte in modo atomico, o
    richieste di lettura su una determinata replica di un blocco invece
    che in un'altra potrebbe portare a risultati inconsistenti o non
    aggiornati.
  \end{itemize}

  Le limitazioni che Hadoop impone sono ragionevoli per lo use-case per
  cui HDFS è progettato, caratterizzato da grandi dataset che vengono
  copiati nel filesystem e letti in blocco.
\item
  \textbf{Dataset di grandi dimensioni}

  I filesystem distribuiti sono generalmente necessari per aumentare la
  capacità di storage disponibile oltre quella di una singola macchina.
  La distribuzione di HDFS, assieme alla grande dimensione dei blocchi
\item
  \textbf{Accesso in streaming}

  HDFS predilige l'accesso ai dati in streaming, per permettere ai
  lavori batch di essere eseguiti con grande efficienza. Questo
  approccio va a discapito del tempo di latenza della lettura dei file,
  ma permette di avere un throughput in lettura molto vicino ai tempi di
  lettura del disco.
\item
  \textbf{Portabilità su piattaforme software e hardware eterogenee}

  HDFS è scritto in Java, ed è portabile in tutti i sistemi che ne
  supportano il runtime.
\end{itemize}

L'architettura di HDFS è di tipo master/slave, dove un nodo centrale,
chiamato \textbf{NameNode}, gestisce i metadati e la struttura del
filesystem, mentre i nodi slave, chiamati \textbf{DataNode}, contengono
i blocchi di cui file sono composti. Tipicamente, viene eseguita
un'istanza del software del DataNode per macchina del cluster, e una
macchina dedicata esegue il NameNode.

I \emph{client} del filesystem interagiscono sia con il NameNode che con
i DataNode per l'accesso ai file. La comunicazione tra il client e i
nodi avviene tramite socket TCP ed è coordinata dal NameNode, che
fornisce ai client tutte le informazioni sul filesystem e su quali nodi
contengono i DataBlock dei file richiesti.

\subsection{Replicazione e
fault-tolerance}\label{replicazione-e-fault-tolerance}

Il blocco è un'astrazione che si presta bene alla replicazione dei dati
nel filesystem all'interno del cluster: per replicare i dati, HDFS
persiste ogni blocco all'interno di più macchine nel cluster. HDFS
utilizza le informazioni sulla configurazione di rete del cluster per
decidere il posizionamento delle repliche di ogni blocco: considerando
che i tempi di latenza di rete sono più bassi tra nodi in uno stesso
rack, HDFS salva due copie del blocco in due nodi che condividono il
rack. In questo modo, nell'eventualità in cui una delle copie del blocco
non fosse disponibile o avesse problemi d'integrità, una sua replica può
essere recuperata in un nodo che si trova all'interno del rack,
minimizzando l'overhead di rete.

Per aumentare la fault-tolerance, HDFS salva un'ulteriore copia del
blocco al di fuori del rack in cui ha memorizzato le prime due. Questa
operazione salvaguardia l'accesso al blocco in caso di fallimento dello
switch di rete del rack che contiene le prime due copie, che renderebbe
inaccessibili tutte le macchine che contieneche contiene.

Il numero di repliche create da HDFS per ogni blocco è definito
\emph{replication factor}, ed è configurabile tramite l'opzione
\lstinline!dfs.replication!. Quando il numero di repliche di un certo
file scende sotto la soglia di questa proprietà (eventualità che accade
in caso di fallimento dei nodi) HDFS riesegue trasparentemente la
replicazione dei blocchi per raggiungere la soglia definita nella
configurazione.

L'integrità dei blocchi è verificata trasparentemente da HDFS alla loro
lettura e scrittura, utilizzando checksum
CRC-32\textbar{}\textbar{}\textbar{}

\subsection{Comunicare con HDFS}\label{comunicare-con-hdfs}

Hadoop fornisce tool e librerie che possono agire da client nei
confronti di HDFS. Il tool più diretto è la CLI, accessibile nelle
macchine in cui è installato Hadoop tramite il comando
\lstinline!hadoop fs!.

\begin{lstlisting}[language=sh]
% hadoop fs -help
Usage: hadoop fs [generic options]
    [-appendToFile <localsrc> ... <dst>]
    [-cat [-ignoreCrc] <src> ...]
    [-checksum <src> ...]
    [-chgrp [-R] GROUP PATH...]
    [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
    [-chown [-R] [OWNER][:[GROUP]] PATH...]
    [-copyFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]
    [-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
    [-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] <path> ...]
    [-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]
...
\end{lstlisting}

La CLI fornisce alcuni comandi comuni nei sistemi POSIX, come
\lstinline!cp!, \lstinline!rm!, \lstinline!mv!, \lstinline!ls! e
\lstinline!chown!, e altri che riguardano specificamente HDFS, come
\lstinline!copyFromLocal! e \lstinline!copyToLocal!, utili a trasferire
dati tra la macchina su cui si opera e il filesystem.

I comandi richiedono l'URI che identifica l'entità su cui si vuole
operare. Per riferirsi a una risorsa all'interno di un'istanza di HDFS,
si usa l'URI del namenode, con schema \lstinline!hdfs!\footnote{Hadoop è
  abbastanza generale da poter lavorare con diversi filesystem, con lo
  schema definisce il protocollo di comunicazione, che non deve essere
  necessariamente \lstinline!hdfs!. Ad esempio, un URI con schema
  \lstinline!file! si riferisce al filesystem locale, e le operazioni
  eseguite su URI che utilizzano questo schema vengono effettuate sulla
  macchina dove viene eseguito il comando. Questo approccio può essere
  adatto nella fase di testing dei programmi, ma nella maggior parte dei
  casi è comunque desiderabile lavorare su un filesystem distribuito
  adeguato alla gestione dei Big Data, e un'alternativa ad HDFS degna di
  nota è MapR-FS{[}@mapr-fs{]}.}, e con il path corrispondente al
percorso della risorsa nel filesystem. Ad esempio, è possibile creare
una cartella \lstinline!foo! all'interno della radice del filesystem con
il seguente comando:

\begin{lstlisting}[language=sh]
hadoop fs -mkdir hdfs://localhost:8020/foo
\end{lstlisting}

Per diminuire la verbosità dei comandi è possibile utilizzare percorsi
relativi, specificando nell'opzione \lstinline!dfs.defaultFS! della
configurazione del cluster l'URI del filesystem ai cui i percorsi
relativi si riferiscono. Gli URI riferiti a istanze di HDFS hanno schema
\lstinline!hdfs://!, seguito dall'indirizzo IP o dell'hostname della
macchina che esegue il NameNode. Specificando l'URI, si può accorciare
l'esempio precedente a:

\begin{lstlisting}[language=sh]
hadoop fs -mkdir foo
\end{lstlisting}

Ad esempio, data la seguente cartella:

\begin{lstlisting}[language=sh]
[root@sandbox example_data]# ls
example1.txt  example2.txt  example3.txt
\end{lstlisting}

Si possono copiare i file dalla cartella locale della macchina al
filesystem distribuito con il seguente comando:

\begin{lstlisting}[language=sh]
[root@sandbox example_data]# hadoop fs -copyFromLocal example*.txt /example
\end{lstlisting}

Per verificare che l'operazione sia andata a buon fine, si può ottenere
un listing della cartella in cui si sono trasferiti i file con il
comando \lstinline!ls!:

\begin{lstlisting}[language=sh]
[root@sandbox example_data]# hadoop fs -ls /example
Found 3 items
-rw-r--r--   1 root hdfs         70 2017-06-30 03:58 /example/example1.txt
-rw-r--r--   1 root hdfs         39 2017-06-30 03:58 /example/example2.txt
-rw-r--r--   1 root hdfs         43 2017-06-30 03:58 /example/example3.txt
\end{lstlisting}

Il listing è molto simile a quello ottenibile su sistemi Unix. Una
differenza importante è la seconda colonna, che non mostra il numero di
hard link al file nel filesystem\footnote{Non è necessario mostrare i
  link dei file, perché HDFS correntemente non li supporta.}, ma il
numero di repliche che HDFS ha a disposizione del file, in questo caso
una per file. Il numero di repliche fatte da HDFS può essere impostato
settando il fattore di replicazione di default, che per Hadoop in
modalità distribuita è 3 di default. Si può anche cambiare il numero di
repliche disponibili per determinati file, utilizzando il comando
\lstinline!hdfs dfs!:

\begin{lstlisting}[language=sh]
[root@sandbox ~]# hdfs dfs -setrep 2 /example/example1.txt
Replication 2 set: /example/example1.txt
[root@sandbox ~]# hadoop fs -ls /example
Found 3 items
-rw-r--r--   2 root hdfs         70 2017-06-30 03:58 /example/example1.txt
-rw-r--r--   1 root hdfs         39 2017-06-30 03:58 /example/example2.txt
-rw-r--r--   1 root hdfs         43 2017-06-30 03:58 /example/example3.txt
\end{lstlisting}

HDFS è anche accessibile tramite \emph{HDFS Web Interface}, un tool che
fornisce informazioni sullo stato generale del filesystem e sul suo
contenuto. Ci sono anche tool di amministrazione di cluster Hadoop che
offrono GUI web più avanzate di quella fornita di default da HDFS. Due
esempi sono Cloudera Manager e Apache Ambari, che offrono un file
manager lato web con cui è possibile interagire in modo più semplice,
permettendo anche a utenti non tecnicamente esperti di lavorare con il
filesystem.

\begin{figure}
\centering
\includegraphics{img/ambari_hdfs.png}
\caption{Screenshot del file manager HDFS incluso in Ambari}
\end{figure}

Un altro importante modo di interfacciarsi ad HDFS è l'API
\lstinline!FileSystem! di Hadoop, che permette un accesso programmatico
da linguaggi per JVM alle funzioni del filesystem. L'API fornisce
interfacce Java generali che possono utilizzata con filesystem diversi
da HDFS, permettendo

Per linguaggi che non supportano interfacce Java, esiste
un'implementazione in C chiamata \lstinline!libhdfs!, che si appoggia
sulla Java Native Interface per esporre l'API di Hadoop.

Esistono poi progetti che permettono il montaggio di HDFS in un
filesystem locale. Alcune di queste implementazioni sono basate su FUSE,
mentre altre su NFS Gateway. Questo metodo di accesso permette
l'utilizzo di utilità native del sistema in uso in HDFS.

\subsection{NameNode in dettaglio}\label{namenode-in-dettaglio}

Il NameNode è il riferimento centrale per i metadati del filesystem nel
cluster, il che vuol dire che se il NameNode non è disponibile il
filesystem non è accessibile. Questo rende il NameNode un \emph{single
point of failure} del sistema, e per questa ragione HDFS mette a
disposizione dei meccanismi per attenutare l'indisponibilità del sistema
in caso di non reperibilità del NameNode, e per assicurare che lo stato
del filesystem possa essere recuperato a partire dal NameNode.

Il NameNode è anche il nodo a cui i client si connettono alla lettura
del file. La connessione ha il solo scopo di fornire le informazioni sui
DataNode che contengono i dati effettivi del file. I dati di un file non
passano mai per il NameNode.

Tuttavia, il NameNode non salva persistentemente le informazioni sulle
posizioni dei blocchi, che vengono invece mantenute dai DataNode. Prima
che il NameNode possa essere operativo, deve ricevere e salvare in
memoria le liste dei blocchi in possesso dei DataNode, in messaggi
chiamati \textbf{block report}. Non è necessario che il DataNode conosca
la posizione di tutti i blocchi sin dall'inizio, ed è sufficiente che
per ogni blocco conosca la posizione di un numero minimo di repliche,
determinato dall'opzione del cluster
\lstinline!dfs.replication.min.replicas!, di default 1.

Questa procedura avviene quando il NameNode si trova in uno stato
chiamato \protect\hyperlink{safe-mode}{\emph{safe mode}}

\subsubsection{\texorpdfstring{\emph{Namespace image} ed \emph{edit
log}}{Namespace image ed edit log}}\label{namespace-image-ed-edit-log}

Le informazioni sui metadati del sistema vengono salvate nello storage
del NameNode in due posti, la \emph{\textbf{namespace image}} e
l'\emph{\textbf{edit log}}. La \emph{namespace image} è uno snapshot
dell'intera struttura del filesystem, mentre l'\emph{edit log} è un
elenco di transazioni eseguite nel filesystem a partire dallo stato
registrato nella \emph{namespace image}. Partendo dalla \emph{namespace
image} e applicando le operazioni registrate nell'\emph{edit log}, è
possibile risalire allo stato attuale del filesystem. Il NameNode ha una
rappresentazione dello stato del filesystem anche nella memoria
centrale, che viene utilizzata per servire le richieste di lettura.

Quando HDFS riceve una richiesta che richiede la modifica dei metadati,
il NameNode esegue le seguenti operazioni:

\begin{enumerate}
\tightlist
\item
  registra la transazione nell'\emph{edit log}
\item
  aggiorna la rappresentazione del filesystem in memoria
\item
  passa all'operazione successiva.
\end{enumerate}

La ragione per cui i cambiamenti dei metadati vengono registrati
nell'\emph{edit log} invece che nella \emph{namespace image} è la
velocità di scrittura: scrivere ogni cambiamento del filesystem mano a
mano che avviene nell'immagine sarebbe lento, dato che questa può avere
dimensioni nell'ordine dei gigabyte. Il NameNode esegue un \emph{merge}
dell'\emph{edit log} e della \emph{namespace image} a ogni suo avvio,
portando lo stato attuale dell'immagine al pari di quello del
filesystem.

Dato che la dimensione dell'\emph{edit log} può diventare notevole, è
utile eseguire l'operazione di \emph{merge} al raggiungimento di una
soglia di dimensione del log. Questa operazione è computazionalmente
costosa, e se fosse eseguita dal NameNode potrebbe interferire con la
sua operazione di routine.

Per evitare interruzioni nel NameNode, il compito di eseguire
periodicamente il \emph{merge} dell'\emph{edit log} è affidato a
un'altra entità, il \textbf{Secondary NameNode}. Il Secondary NameNode
viene solitamente eseguito su una macchina differente, dato che richiede
un'unità di elaborazione potente e almeno la stessa memoria del NameNode
per eseguire l'operazione di merge.

\hypertarget{safe-mode}{\subsubsection{\texorpdfstring{Avvio del
NameNode e \emph{Safe
Mode}}{Avvio del NameNode e Safe Mode}}\label{safe-mode}}

Prima di essere operativo, il NameNode deve eseguire alcune operazioni
di startup, tra cui attendere di aver ricevuto i block report dai
DataNode in modo da conoscere le posizioni dei blocchi. Durante queste
operazioni, il NameNode si trova in uno stato chiamato \emph{safe mode},
in cui sono permesse unicamente operazioni che accedono ai metadati del
filesystem, e tentativi di lettura e scrittura di file falliscono. Prima
di poter permettere l'accesso completo, il NameNode ha bisogno di
ricevere le informazioni sui blocchi da parte dei DataNode.

Per ricapitolare, al suo avvio, il NameNode effettua il merge della
\emph{namespace image} con l'\emph{edit log}. Al termine
dell'operazione, il risultato del merge viene salvato come la nuova
\emph{namespace image}. Il Secondary NameNode non viene coinvolto in
questo primo merge.

Prima di uscire dalla safe mode, il NameNode attende di avere abbastanza
informazioni da poter accedere a un numero minimo di repliche di ogni
blocco. A questo punto il NameNode esce dalla safe mode.

Si possono utilizzare dei comandi per verificare lo stato, attivare e
disattivare la safe mode.

\begin{lstlisting}[language=sh]
bash-4.1$ hdfs dfsadmin -safemode get
Safe mode is OFF
bash-4.1$ hdfs dfsadmin -safemode enter
Safe mode is ON
bash-4.1$ hdfs dfsadmin -safemode leave
Safe mode is OFF
\end{lstlisting}

\begin{figure}
\centering
\includegraphics{img/hdfs-web-startup.png}
\caption{Lo stato dello startup di un'istanza di HDFS, mostrata da HDFS
Web Interface.}
\end{figure}

\subsection{Processo di lettura di file in
HDFS}\label{processo-di-lettura-di-file-in-hdfs}

\begin{figure}
\centering
\includegraphics{img/hdfs-file-read.png}
\caption{Diagramma delle operazioni eseguite nella lettura di un file in
HDFS{[}@hadoop-guide-hdfs-file-read{]}}
\end{figure}

Per avere un quadro completo del funzionamento di HDFS, è utile
osservare come avvenga il processo di lettura di un file. In questa
sezione si prende in esame un programma di esempio che utilizza le API
\lstinline!FileSystem! di Hadoop per reimplementare una versione
semplificata del comando \lstinline!cat!, per poi esaminare come le
operazioni specificate nel programma vengano effettivamente portate a
termine in un'istanza di HDFS.

\begin{codelisting}

\caption{Programma di esempio che reimplementa il comando
\lstinline!cat!.}

\begin{lstlisting}[language=Java, numbers=left, label=lst:hdfs-cat]
import java.io.InputStream;
import java.net.URI;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

public class MyCat {

    public static void main(String args[]) throws Exception {

        String source = args[0];
        Configuration conf = new Configuration();

        try(
            FileSystem sourcefs = FileSystem.get(URI.create(source), conf);
            InputStream in = sourcefs.open(new Path(source))
        ) {
            IOUtils.copyBytes(in, System.out, 4096, false);
        }
    }
}
\end{lstlisting}

\end{codelisting}

La reimplementazione del programma \lstinline!cat! utilizza il primo
parametro della linea di comando per ricevere l'URI del file che si
vuole stampare nello standard output. L'URI deve contenere il percorso
di rete del filesystem HDFS, ed essere quindi del formato
\lstinline!hdfs://[indirizzo o hostname del namenode]/[path del file]!.
Di seguito vengono spiegati i passi eseguiti dal programma. Quando non
qualificato, l'identificativo \lstinline!hadoop! si riferisce al package
Java \lstinline!org.apache.hadoop!.

\begin{enumerate}
\item
  Si crea un oggetto \lstinline!hadoop.conf.Configuration!. Gli oggetti
  \lstinline!Configuration! forniscono l'accesso ai parametri di
  configurazione di Hadoop (impostati in file XML, come descritto in
  \protect\hyperlink{installazione-e-configurazione}{Installazione e
  Configurazione}).
\item
  Si ottiene un riferimento \lstinline!sourcefs! a un
  \lstinline!hadoop.fs.FileSystem! (dichiarato come interfaccia Java),
  che fornisce le API che verranno usate per leggere e manipolare il
  filesystem. Il riferimento viene ottenuto tramite il metodo statico
  \lstinline!FileSystem.get(URI source, Configuration conf)!, che
  richiede un URI che possa essere utilizzato per risalire a quale
  filesystem si vuole accedere. Un overload di
  \lstinline!FileSystem.get! permette di specificare solo l'oggetto
  \lstinline!Configuration!, e ottiene le informazioni sul filesystem da
  aprire dalla proprietà di configurazione \lstinline!dfs.defaultFS!.
\item
  Si apre il file il lettura, chiamando
  \lstinline!sourcefs.open(Path file)!. Il metodo restituisce un oggetto
  di tipo \lstinline!hadoop.fs.FSDataInputStream!, una sottoclasse di
  \lstinline!java.io.InputStream! che supporta anche l'accesso a punti
  arbitrari del file. In questo case l'oggetto è utilizzato per leggere
  il file sequenzialmente, e il suo riferimento viene salvato nella
  variabile \lstinline!InputStream in!.
\item
  Si copiano i dati dallo stream \lstinline!in! a
  \lstinline!System.out!, di fatto stampando i dati nella console.
  Questa operazione è eseguita tramite il metodo
  \lstinline!hadoop.io.IOUtils.copyBytes(InputStream in, OutputStream out, int bufSize, bool closeStream)!.
  Il metodo copia i dati da uno stream d'ingresso a uno d'uscita, e non
  ha funzioni specifiche rispetto ad Hadoop, ma viene fornito per la
  mancanza di un meccanismo simile in Java.
\item
  Lo stream e l'oggetto \lstinline!FileSystem! vengono chiusi.
  L'operazione avviene implicitamente utilizzando il costrutto
  try-with-resources di Java.
\end{enumerate}

L'esecuzione del programma dà il seguente output:

\begin{lstlisting}[language=sh]
$~ hadoop MyCat hdfs://sandbox.hortonworks.com:8020/example/example1.txt
This is the first example file
\end{lstlisting}

Nel caso di un URI con schema HDFS, l'istanza concreta di
\lstinline!FileSystem! che viene restituita da
\lstinline!FileSystem.get! è di tipo \lstinline!DistributedFileSystem!,
che contiene le funzionalità necessarie a comunicare con HDFS. Con uno
schema diverso (ad esempio \lstinline!file://! per filesystem locali),
l'istanza concreta di \lstinline!FileSystem! cambia per gestire
opportunamente lo schema richiesto (se supportato).

Dietro le quinte, \lstinline!FSDataInputStream!, restituito da
\lstinline!FileSystem.open(...)!, utilizza chiamate a procedure remote
sul namenode per ottenere le posizioni dei primi blocchi del file. Per
ogni blocco, il namenode restituisce gli indirizzi dei datanode che lo
contengono, ordinati in base alla prossimità del client. Se il client
stesso è uno dei datanode che contiene un blocco da leggere, il blocco
viene letto localmente.

Alla prima chiamata di \lstinline!read()! su
\lstinline!FSDataInputStream!, l'oggetto si connette al DataNode che
contiene il primo blocco del file, e lo richiede (nell'esempio,
\lstinline!read! viene chiamato da \lstinline!IOUtils.copyBytes!). Il
DataNode risponde inviando i dati corrispondenti al blocco, fino al
termine di questi. Al raggiungimento della fine di un blocco,
\lstinline!DFSInputStream! termina la connessione con il DataNode
corrente e ne inizia un'altra con il più prossimo dei DataNode che
contiene il blocco successivo.

In caso di errore dovuto al fallimento di un DataNode o alla ricezione
di un blocco di dati corrotto, il client può ricevere un'altra copia del
blocco dal nodo successivo della lista di DataNode ricevuta dal
NameNode.

I blocchi del file non vengono inviati tutti insieme, e il client deve
periodicamente richiedere al NameNode i dati sui blocchi successivi.
Questo passaggio avviene trasparentemente rispetto all'interfaccia, in
cui l'utilizzatore si limita a chiamare \lstinline!read! su
\lstinline!DFSInputStream!.

Le comunicazioni di rete, in questo meccanismo, sono distribuite su
tutto il cluster. Il NameNode riceve richieste che riguardano solo i
metadati dei file, mentre il resto delle connessioni viene eseguito
direttamente tra client e DataNode. Questo approccio permette ad HDFS di
evitare colli di bottiglia dovuti a un punto di connessione ai client
centralizzato, distribuendo le comunicazioni di rete attraverso i vari
nodi del cluster.

\section{YARN}\label{yarn}

YARN è acronimo di Yet Another Resource Negotiator, ed è l'insieme di
API su cui sono implementati framework di programmazione distribuita di
livello più alto, come MapReduce e Spark. YARN si definisce un
\emph{``negotiator''} perché è l'entità che decide quando e come le
risorse del cluster debbano essere allocate per l'esecuzione
distribuita, e che gestisce le comunicazioni che riguardano le risorse
con tutti i nodi coinvolti. Inoltre, YARN ha l'importante ruolo di
esporre un'interfaccia che permette di imporre \textbf{vincoli di
località} sulle risorse richieste dalle applicazioni, permettendo
l'implementazione di applicazioni che seguono il principio di \emph{data
locality} di Hadoop.

I servizi di YARN sono offerti tramite \emph{demoni} eseguiti nei nodi
del cluster. Ci sono due tipi di demoni in YARN:

\begin{itemize}
\item
  i \textbf{NodeManager}, che eseguono su richiesta i processi necessari
  allo svolgimento dell'applicazione nel cluster. L'esecuzione dei
  processi avviene attraverso \emph{container}, che permettono di
  limitare le risorse utilizzate da ogni processo eseguito. Il
  NodeManager viene eseguito in ogni nodo del cluster che prende parte
  alle computazioni distribuite.
\item
  il \textbf{ResourceManager}, di cui è eseguita un'istanza per cluster,
  e che gestisce le sue risorse. Il ResourceManager è l'entità che
  comunica con i NodeManager e che decide quali processi questi debbano
  eseguire e quando.
\end{itemize}

I container in YARN possono essere rappresentativi di diverse modalità
di esecuzione di un processo. Queste sono configurabili dall'utente
tramite la proprietà
\lstinline!yarn.nodemanager.container-executor.class!, il cui valore
identifica una classe che stabilisce come i processi debbano essere
eseguiti. Di default, l'esecuzione utilizza normali processi UNIX, ma la
configurazione permette l'uso di container di virtualizzazione OS-level,
come lxc e Docker.

L'esecuzione di applicazioni distribuite in YARN è richiesta dai client
al ResourceManager. Quando il ResourceManager decide di avviare
un'applicazione, alloca un container in uno dei NodeManager e lo
utilizza per invocare un \textbf{application master}.

L'application master è specificato dalle singole applicazioni, ed ha i
seguenti ruoli{[}@hortonworks-yarn{]}:

\begin{itemize}
\tightlist
\item
  negoziare l'acquisizione di nuovi container con il ResourceManager nel
  corso dell'applicazione;
\item
  utilizzare i container per eseguire i processi distribuiti di cui è
  costituita l'applicazione;
\item
  monitorare lo stato e il progresso dell'esecuzione dei processi nei
  container.
\end{itemize}

Le richieste di container specificano CPU, memoria, e la specifica
macchina dove si desidera l'esecuzione. Tra i parametri della richiesta
è anche possibile specificare se si vuole permettere l'esecuzione in una
macchina diversa da quella richiesta, qualora non fosse disponibile.

Mano a mano che la computazione procede, l'application master può
riferire al ResourceManager di rilasciare determinate risorse. Quando il
master decide di porre termine all'applicazione, lo riferisce al
ResourceManager, in modo da permettere il rilascio del container in cui
è eseguito.

Il ResourceManager è in grado di gestire più job contemporaneamente
utilizzando diverse politiche di scheduling. L'esecuzione dei job può
essere richiesta da diversi utenti ed entità che hanno accesso al
cluster, e la scelta di una politica di scheduling adeguata permette di
stabilire priorità di accesso diverse per ognuna delle entità.

Tra gli scheduler forniti da Hadoop, i seguenti sono i più utilizzati:

\begin{itemize}
\item
  Lo scheduler \textbf{FIFO} esegue i job sequenzialmente in ordine di
  arrivo, e ogni job può potenzialmente utilizzare tutte le risorse del
  risorse del cluster.
\item
  Il \textbf{Fair Scheduler} esegue i job concorrentemente, fornendo una
  parte delle risorse del cluster a ogni job. I job possono avere una
  \emph{priorità}, ovvero un peso che determina la frazione di risorse
  che ricevono. Mano a mano che nuovi job arrivano, le risorse
  rilasciate dai job già in esecuzione vengono riassegnate al nuovo job
  per bilanciare la distribuzione delle risorse in base ai
  pesi{[}@yarn-fair-scheduler{]}. È anche possibile configurare lo
  scheduler in modo che le risorse siano distribuite in base agli utenti
  che richiedono l'esecuzione dei job.
\item
  Il \textbf{Capacity Scheduler} è il più adatto per condividere cluster
  tra organizzazioni. Lo scheduler viene configurato per avere diverse
  \emph{code gerarchiche} di job, ognuna dedicata a un ente che fa uso
  del cluster. Per ogni coda è specificata una quantità minima di
  risorse del cluster che devono essere disponibili per l'uso in ogni
  momento, di cui lo scheduler garantisce la disponibilità.
\end{itemize}

Gli scheduler sono implementati in classi, e lo scheduler da istanziare
viene scelto dal ResourceManager cercando, tramite reflection Java, la
classe con il nome indicato in
\lstinline!yarn.resourcemanager.scheduler.class!. L'utente è libero di
implementare un proprio scheduler e di specificarne l'identificatore in
questa proprietà.

\chapter{Batch Processing}\label{batch-processing}

Il Batch Processing è la \emph{raison d'être} di Hadoop. Il primo
paradigma di programmazione per Hadoop, MapReduce, è stato l'unico per
molte release, e ha avuto il grande merito di astrarre la complessità
della computazione batch in ambiente distribuito in funzioni che
associano chiavi e valori a risultati, una grande semplificazione
rispetto ai programmi che gestiscono granularmente l'intricatezza di
ambienti distribuiti.

Pur essendo popolare, MapReduce è soggetto a molte limitazioni, che
riguardano soprattutto la necessità di esprimere i programmi da eseguire
con un modello che non lascia molto spazio alla rielaborazione dei
risultati. Come si vedrà, queste limitazioni sono intrinseche al fatto
che i risultati intermedi vengano salvati nello storage locale del nodo
del cluster, e quelli finali in HDFS. Questi due fattori influenzano
pesantemente le prestazioni che si possono ottenere da un algoritmo,
perché vi introducono l'overhead della lettura e scrittura nel disco, o
peggio in HDFS.

YARN è stato creato proprio per questo motivo: permettere che altri
modelli di computazione diversi da MapReduce potessero essere eseguiti
sfruttando HDFS. Le nuove versioni di MapReduce sono implementate al di
sopra di YARN invece che direttamente in Hadoop come in passato, a
testimoniare l'effettiva capacità di YARN di generalizzare i modelli di
esecuzione nei cluster.

La sua alternativa più popolare, Apache Spark, ha API più espressive e
funzionali rispetto a MapReduce, ed è più performante in molti tipi di
algoritmi{[}@mapreduce-spark-performance{]}. Tramite astrazioni che
offrono un controllo più preciso sul comportamento dei risultati
dell'elaborazione, Spark trova applicazioni pratiche in vari ambiti, tra
cui machine learning{[}@spark-mllib{]}, graph
processing{[}@spark-graphx{]} e elaborazione SQL{[}@spark-sql{]}.

In questa sezione si esaminano MapReduce e Spark, quali sono le
limitazioni di MapReduce che hanno fatto sentire la necessità di un
nuovo modello computazionale, e quali sono le soluzioni offerte da
Spark. Si accenneranno anche ad alcune astrazioni fatte al di sopra di
MapReduce, come Pig e Hive, che forniscono dei modelli computazionali
che vengono tradotti in job MapReduce.

\section{MapReduce}\label{mapreduce}

Il modello computazionale di MapReduce è composto, nella sostanza, da
due componenti, il Mapper e il Reducer. Questi componenti sono
specificati dall'utilizzatore del framework, e possono essere descritti
come due funzioni.

\[Map(K_1, V_1) \mapsto Sequence[(K_2, V_2)]\]

La funzione \(Map\) è eseguita nello stadio iniziale della computazione
su valori di input esterni. L'input della funzione \(Map\) è una coppia
chiave-valore \(K_1\) e \(V_1\), i cui valori dipendono dal tipo di
input letto. Ad esempio, nei file di testo, \(K_1\) rappresenta il
numero di riga di un file e \(V_1\) la riga di testo corrispondente.

A partire da ogni coppia, \(Map\) elabora e restituisce una sequenza di
nuove coppie chiave-valore di tipo \(K_2\) e \(V_2\). Queste coppie
vengono poi rielaborate trasparentemente dal framework, che esegue due
operazioni:

\begin{enumerate}
\tightlist
\item
  \textbf{ordina} tutte le coppie in base alla chiave;
\item
  \textbf{aggrega} le coppie che condividono la stessa chiave in una
  nuova coppia \((K_2, Sequence[V_2])\).
\end{enumerate}

\[Reduce(K_2, Sequence[V_2]) \mapsto (K_2, V_3)\]

Ognuna delle coppie aggregate dal framework viene poi fornita in input
alla funzione \(Reduce\), che ha quindi a disposizione una chiave
\(K_2\) e tutti i valori restituiti da \(Map\) che hanno la stessa
chiave \(K_2\). \(Reduce\) esegue una computazione sui valori di input e
restituisce \((K_2, V_3)\), che andrà a far parte dell'output finale
dell'applicazione assieme al risultato delle altre invocazioni di
\(Reduce\), una per ogni chiave distinta restituita da \(Map\).

Sintetizzando, MapReduce permette di categorizzare l'input in diverse
parti e di elaborare un risultato per ognuna di queste.

MapReduce è quindi un paradigma \emph{funzionale}, dato che il framework
richiede di ricevere in input le funzioni utili all'elaborazione dei
dati. Per esprimere questo tipo di paradigma in Java si ricorre a classi
che incapsulano le funzioni richieste dal framework, che vengono quindi
chiamate Mapper e Reducer.

Il Mapper in un'applicazione MapReduce è una classe contenente un metodo
\lstinline!void map!, che riceve in input una chiave e un valore, e un
oggetto \lstinline!Context!, il cui ruolo più importante è fornire il
metodo \lstinline!Context.write(K, V)!, che viene utilizzato per
scrivere i valori di output del Mapper.

Le applicazioni MapReduce specificano un proprio Mapper estendendo la
classe \lstinline!Mapper! nella libreria di Hadoop, e compilando i tipi
dei parametri generici opportunamente. La firma di \lstinline!Mapper! è
la seguente:

\begin{lstlisting}[language=Java]
public class Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> extends Object
\end{lstlisting}

Le chiavi e i valori ricevuti in input dal Mapper sono derivati
direttamente dall'elemento letto in HDFS. È possibile configurare quali
chiavi e valori vengano derivati dalla sorgente e come, creando una
classe che implementa l'interfaccia \lstinline!InputMapper! fornita
nella libreria di Hadoop. Nella libreria, Hadoop fornisce diversi
\lstinline!InputMapper! che corrispondono a comportamenti di lettura
desiderabili per diversi tipi di file e sorgenti, come file con formati
colonnari, o contenti coppie chiave-valore divise da marcatori.

I tipi ricevuti in input dal Mapper sono specificati nei parametri
generici \lstinline!KEYIN! e \lstinline!VALUEIN!, e devono corrispondere
ai tipi che l'\lstinline!InputFormat! di riferimento restituisce.
\lstinline!KEYOUT! e \lstinline!VALUEOUT! sono invece i tipi che il
Mapper restituisce rielaborando le chiavi e i valori in input.
\lstinline!map! ha la seguente signature:

\begin{lstlisting}[language=Java]
protected void map(KEYIN key, VALUEIN value, Context context) 
    throws IOException, InterruptedException
\end{lstlisting}

Una volta restituiti dal Mapper, le coppie vengono date in input a una
classe \lstinline!Reducer!, che ha una signature simile a quella del
\lstinline!Mapper!:

\begin{lstlisting}[language=Java]
public class Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> extends Object
\end{lstlisting}

Una classe che estende \lstinline!Reducer! ha un metodo
\lstinline!reduce!, che riceve in input una chiave, e un iterabile di
tutti i valori restituiti dai Mapper che hanno quella stessa chiave:

\begin{lstlisting}[language=Java]
protected void reduce(KEYIN key, Iterable<VALUEIN> values, Context context) 
    throws IOException, InterruptedException
\end{lstlisting}

Nella fase di reduce, quindi, i valori sono aggregati in base alla
chiave e resi disponibili tramite l'interfaccia \lstinline!Iterable! di
Java. I valori a questo punto possono essere combinati a seconda
dell'esigenza dell'utente per restituire un risultato finale.

\subsection{Esempio di un programma
MapReduce}\label{esempio-di-un-programma-mapreduce}

Come esempio di programma per MapReduce, si prende in considerazione
l'analisi di log di un web server. Il dataset su cui si esegue
l'elaborazione è fornito liberamente dalla NASA{[}@nasa-weblog{]}, e
corrisponde ai log di accesso al server HTTP del Kennedy Space Center
dal 1/07/1995 al 31/07/1995. Il log è un file di testo con codifica
ASCII, dove ogni riga corrisponde a una richiesta e contiene le seguenti
informazioni:

\begin{enumerate}
\tightlist
\item
  L'host che esegue la richiesta, sotto forma di hostname quando
  disponibile o indirizzo IP altrimenti;
\item
  Timestamp della richiesta, in formato
  ``\lstinline!WEEKDAY MONTH DAY HH:MM:SS YYYY!'' e fuso orario, con
  valore fisso \lstinline!-0400!;
\item
  La Request-Line HTTP tra virgolette;
\item
  Il codice HTTP di risposta;
\item
  La dimensione in byte della risposta.
\end{enumerate}

\begin{codelisting}

\caption{Campione di due righe dal log da analizzare}

\begin{lstlisting}[label=lst:log-sample]
ntp.almaden.ibm.com - - [24/Jul/1995:12:40:12 -0400] 
    "GET /history/apollo/apollo.html HTTP/1.0" 200 3260

fsd028.osc.state.nc.us - - [24/Jul/1995:12:40:12 -0400]
    "GET /shuttle/missions/missions.html HTTP/1.0" 200 8678
\end{lstlisting}

\end{codelisting}

A partire da questo log, si vuole capire quante richieste siano state
ricevute da ogni risorsa HTTP. Un possibile approccio alla risoluzione
del problema è eseguire il parsing di ogni riga del log nel Mapper
utilizzando un'espressione regolare, per estrarre l'URI dalla richiesta.
Il Mapper, per ogni riga, restituische l'URI come chiave e 1 come
valore.

Dopo l'esecuzione dei Mapper, i Reducer riceveranno una coppia formata
dall'URI delle richieste come chiave, e da un iterabile di valori 1, uno
per ogni richiesta. È sufficiente sommare questi valori per ottenere il
numero di richieste finale per l'URI chiave.

\begin{codelisting}

\caption{Implementazione del Mapper utilizzato per analizzare il file di
log.}

\begin{lstlisting}[language=Java, label=lst:log-mapper]
import java.io.IOException;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

public class LogMapper extends Mapper<LongWritable, Text, Text, LongWritable> {

    private final static Pattern logPattern = Pattern.compile(
        ".*\"[A-Z]+ (.*) HTTP.*"
    );

    private final static LongWritable one = new LongWritable(1);

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        final String request = value.toString();
        final Matcher requestMatcher = logPattern.matcher(request);

        if(requestMatcher.matches()) {
            context.write(
                new Text(requestMatcher.group(1)),
                one
            );
        }
    }

}
\end{lstlisting}

\end{codelisting}

Come si può osservere da lst.~\ref{lst:log-mapper}, i tipi utilizzati
dal Mapper non sono tipi standard Java, ma sono forniti dalla libreria.
Hadoop utilizza un suo formato di serializzazione per lo storage e per
la trasmissione dei dati in rete, diverso dalla serializzazione
integrata in Java. In questo modo il framework ha controllo preciso
sulla fase di serializzazione, un fattore importante data la crucialità
in termini di efficienza che questa può avere.

Le funzionalità di serializzazione di Hadoop sono rese accessibili dagli
oggetti serializzabili tramite l'interfaccia
\lstinline!hadoop.io.Writable!. Le classi \lstinline!LongWritable! e
\lstinline!Text! sono dei wrapper sui tipi \lstinline!long! e
\lstinline!String! che implementano l'interfaccia \lstinline!Writable!,
e i valori contenuti in questi tipi possono essere ottenuti
rispettivamente con \lstinline!LongWritable.get()! e
\lstinline!Text.toString()!\footnote{Le classi definite dagli utenti
  possono implementare a loro volta l'interfaccia \lstinline!Writable!
  per essere supportate come tipi di chiavi e valori nei Mapper e nei
  Reducer.}.

Nel Mapper, si utilizza l'espressione regolare
\lstinline!/.*"[A-Z]+ (.*) HTTP.*/! per ottenere il token contenente
l'URI della richiesta, e tramite \lstinline!context.write! si
restituisce la coppia URI e 1.

\begin{codelisting}

\caption{Implementazione del Reducer per il programma di analisi dei
log.}

\begin{lstlisting}[language=Java, label=lst:log-reducer]

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class LogReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
    @Override
    protected void reduce(Text key, Iterable<LongWritable> values, Context context)
            throws IOException, InterruptedException {

        long accumulator = 0;

        for(LongWritable value: values) {
            accumulator += value.get();
        }

        context.write(key, new LongWritable(accumulator));

    }
}
\end{lstlisting}

\end{codelisting}

Il Reducer, mostrato in lst.~\ref{lst:log-reducer}, prende in input nel
suo metodo \lstinline!reduce! i valori aggregati in base alla chiave.
Una volta sommati in una variabile accumulatore, questi vengono scritti
in output in una coppia URI-accumulatore. L'insieme di tutte le coppie
restituite dal Reducer costituiscono l'output finale del programma, che
vengono scritte in un file di testo separando le chiavi dai valori con
caratteri di tabulazioni, e ogni valore di restituzione con un nuova
riga.

\begin{figure}
\centering
\includegraphics[width=0.75000\textwidth]{img/mapreduce_diagram.png}
\caption{Diagramma di funzionamento di MapReduce{[}@mapred-diagram{]}}
\end{figure}

Prima di poter eseguire l'applicazione, è necessario creare un
esecutore, ovvero una classe contenente un punto d'entrata
\lstinline!main! che utilizzi le API di Hadoop per eseguire il
programma, analogamente a come descritto in
\protect\hyperlink{esecuzione-di-software-in-hadoop}{Esecuzione di
software in Hadoop}. I lavori MapReduce sono configurati tramite
l'oggetto \lstinline!hadoop.mapreduce.Job!, che richiede di specificare
le classi da utilizzare come Mapper e Reducer, assieme ai percorsi dei
file da elaborare. L'esecutore dell'analizzatore di log è mostrato in
lst.~\ref{lst:log-executor}.

\begin{codelisting}

\caption{Esecutore dell'analizzatore di log.}

\begin{lstlisting}[language=Java, label=lst:log-executor]

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class LogAnalyzer {

    public static void main(String args[]) throws Exception {
        if(args.length != 2) {
            System.err.println("Usage: LogAnalyzer <input path> <output path>");
        }

        Job job = Job.getInstance();
        job.setJarByClass(LogAnalyzer.class);
        job.setJobName("LogAnalyzer");

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setMapperClass(LogMapper.class);
        job.setReducerClass(LogReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
\end{lstlisting}

\end{codelisting}

L'oggetto \lstinline!job! è il centro della configurazione del programma
MapReduce. Tramite questo si specificano il \lstinline!jar! contenente
le classi dell'applicazione, il nome del Job, utilizzato per mostrare
descrittivamente nei log e nell'interfaccia web lo stato di
completamente di questo, le classi Mapper e Reducer e i tipi dei valori
di output del Reducer. Vengono impostati anche i path del file di input
e dei file di output, utilizzando i valori ricevuti come parametri in
\lstinline!args!. Il job viene effettivamente eseguito alla chiamata di
\lstinline!job.waitForCompletion(bool verbose)!, che restituisce
\lstinline!true! quando questo va a buon fine.

Al termine della compilazione e pacchettizzazione, il programma può
essere eseguito con il comando \lstinline!hadoop!:

\begin{lstlisting}[language=sh]
$ hadoop LogAnalyzer /example/NASA_access_log_Jul95 /example/LogAnalyzerOutput
\end{lstlisting}

Il metodo \lstinline!job.waitForCompletion! è stato invocato con il
parametro \lstinline!verbose! impostato a \lstinline!true!, per cui
l'esecuzione stampa in output un log sul job in esecuzione. Lo stato di
esecuzione dei job è anche consultabile tramite un'interfaccia web
fornita dal framework.

\begin{lstlisting}[language=sh]
17/07/03 18:17:47 INFO Configuration.deprecation: session.id is deprecated.
    Instead, use dfs.metrics.session-id
17/07/03 18:17:47 INFO jvm.JvmMetrics: Initializing JVM Metrics with 
    processName=JobTracker, sessionId=
17/07/03 18:17:47 WARN mapreduce.JobResourceUploader: Hadoop command-line 
    option parsing not performed. Implement the Tool interface and execute
    your application with ToolRunner to remedy this.
17/07/03 18:17:48 INFO input.FileInputFormat: Total input files to process : 1
17/07/03 18:17:48 INFO mapreduce.JobSubmitter: number of splits:2
17/07/03 18:17:48 INFO mapreduce.JobSubmitter: Submitting tokens for 
    job: job_local954245035_0001
17/07/03 18:17:48 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
17/07/03 18:17:48 INFO mapreduce.Job: Running job: job_local954245035_0001
17/07/03 18:17:48 INFO mapred.LocalJobRunner: OutputCommitter set in config null
...
\end{lstlisting}

Al termine dell'esecuzione, i risultati sono disponibili in HDFS nella
cartella \lstinline!/example/LogAnalyzerOutput!, come specificato nei
parametri d'esecuzione. I risultati si trovano in una cartella perché
possono essere composti da più file, uno per ogni Reducer eseguito
parallelamente dal framework. Questa limitazione è dovuta ad HDFS, che
restringe rigidamente l'accesso in scrittura ai file a un solo
utilizzatore. In questo caso, il job è stato eseguito da un solo
reducer, per cui i risultati si trovano in un unico file. L'output dei
reducer è salvato in file testuali con il nome \lstinline!part-r-!
seguito da un numero sequenziale che identifica l'istanza del Reducer
che lo ha prodotto.

Eseguendo \lstinline!ls! nella cartella di output si può effettivamente
verificare la presenza del file prodotto dal Reducer.

\begin{lstlisting}[language=sh]
$ hadoop fs -ls /example/LogAnalyzerOutput
Found 2 items
-rw-r--r--   3 heygent hdfs          0 2017-07-03 18:17 /example/.../_SUCCESS
-rw-r--r--   3 heygent hdfs     804597 2017-07-03 18:17 /example/.../part-r-0000
\end{lstlisting}

Assieme al risultato della computazione, MapReduce salva un file vuoto
chiamato \lstinline!_SUCCESS!, utilizzabile per verificare
programmaticamente se il job è andato a buon fine. Consultando il file,
si può osservare il risultato della computazione eseguita.

\begin{lstlisting}[language=sh]
...
/elv/DELTA/del181.gif   71
/elv/DELTA/del181s.gif  390
/elv/DELTA/deline.gif   84
/elv/DELTA/delseps.jpg  90
/elv/DELTA/delta.gif    1492
/elv/DELTA/delta.htm    267
/elv/DELTA/deprev.htm   71
/elv/DELTA/dsolids.jpg  84
/elv/DELTA/dsolidss.jpg 369
/elv/DELTA/euve.jpg     36
/elv/DELTA/euves.jpg    357
/elv/DELTA/rosat.jpg    38
/elv/DELTA/rosats.jpg   366
/elv/DELTA/uncons.htm   163
...
\end{lstlisting}

\clearpage

\subsection{Modello di esecuzione di
MapReduce}\label{modello-di-esecuzione-di-mapreduce}

Il modello di programmazione di MapReduce è progettato per essere
altamente parallelizzabile e in modo che sia possibile processare
diverse parti dell'input indipendentemente. Questo dato si riflette nel
design del Mapper, che riceve come input piccole porzioni del file
letto, permettendo al framework di assegnare l'elaborazione delle
operazioni di Map a diversi processi indipendenti.

MapReduce è implementato in YARN, e utilizza le sue astrazioni per
avvantaggiarsi della località dei dati, eseguendo i processi che
riguardano una certa porzione di input nei nodi che contengono i
corrispondenti blocchi HDFS. L'esecuzione dei lavori MapReduce avviene
secondo i seguenti step:

\begin{enumerate}
\item
  I file vengono partizionati da MapReduce in frammenti chiamati
  \emph{split}, e per ognuno di questi MapReduce esegue un \emph{map
  task} in un determinato nodo del cluster. Ogni map task può eseguire
  uno o più processi nel nodo in cui si trova, a seconda delle risorse
  assegnate da YARN.

  La dimensione degli split è configurabile, e non corrisponde
  necessariamente alla dimensione di un blocco HDFS, pur essendo questa
  l'opzione di default (128 MB). Con split della stessa dimensione dei
  blocchi, la maggior parte dei dati può essere processata dai nodi che
  contengono il blocco nel loro storage locale. È possibile configurare
  MapReduce per utilizzare split più grandi, ma se una parte dello split
  non si trova nel nodo in cui viene eseguito il map task, questa deve
  essere ricevuta tramite rete da un altro nodo nel cluster che la
  contiene, riducendo quindi la \emph{data locality}.
\item
  In ogni \emph{map task}, lo split corrispondente viene diviso in più
  \emph{record}, che corrispondono alle coppie ricevute in input dal
  Mapper. Il map task esegue il Mapper in uno o più processi del nodo in
  cui si trova, per poi salvare il loro output nello storage locale del
  nodo che esegue il map task. Lo storage locale è più efficiente per la
  scrittura, ma non offre fault-tolerance, per cui in caso di fallimento
  del nodo che contiene i risultati di un \emph{map task}, l'application
  master dell'applicazione MapReduce deve schedulare la sua
  riesecuzione.

  Oltre all'esecuzione dei Mapper, i nodi in questa fase ordinano la
  parte di output in loro possesso in base alla chiave. Questo permette
  di eseguire parallelamente buona parte del sorting dell'input dei
  Reducer.
\item
  Quando non ci sono più \emph{map task} da eseguire sull'input,
  l'application master inizia ad avviare i \emph{reduce task}. I
  \emph{reduce task} ricevono in input i risultati ordinati prodotti dai
  \emph{map task} mano a mano che questi sono disponibili. Ogni
  \emph{map task} può inviare coppie chiave-valore a ogni \emph{reduce
  task}, a condizione che coppie con la stessa chiave finiscano sempre
  nello stesso reducer.

  Per fare questo, i nodi che eseguono \emph{map task} dividono il loro
  output in partizioni, una per ogni Reducer. Ogni chiave delle coppie
  di output viene associata univocamente a una partizione, utilizzando
  la seguente funzione:

  \[partitionId(K_i) = hashCode(K_i) \bmod partitionCount\]

  In questo modo, la stessa chiave è sempre associata alla stessa
  partizione in ogni nodo.
\item
  I nodi che eseguono i Reducer ricevono dai nodi Mapper diversi insiemi
  ordinati di coppie chiave-valore. Questi gruppi vengono uniti tramite
  un'operazione di \emph{merge}, analoga alla stessa operazione nel
  contesto del \emph{mergesort}. Una volta ricevuti tutti i valori, il
  \emph{reduce task} esegue i Reducer che computano l'output finale
  dell'applicazione.
\end{enumerate}

\section{Spark}\label{spark}

Le astrazioni fornite dal paradigma computazionale di MapReduce tolgono
dall'utente l'onere di pensare al dataset in elaborazione, astraendo
l'applicazione a una serie di elaborazioni su chiavi e valori. Questa
astrazione ha tuttavia un costo: l'utente non ha il controllo sulla
gestione del flusso dei dati, che è affidata interamente dal framework.

Il costo della semplificazione diventa evidente quando si cerca di
utilizzare MapReduce per eseguire operazioni che richiedono la
rielaborazione di risultati. Al termine di ogni job MapReduce, l'output
viene salvato in HDFS, ed è quindi necessario rileggerlo dal filesystem
per poterlo utilizzare.

Di per sé, MapReduce non contiene un meccanismo che permetta la
schedulazione consecutiva di job che ricevono in input l'output di un
altro job, e per eseguire elaborazioni che richiedono più fasi è
necessario utilizzare tool esterni. Inoltre, l'overhead della lettura e
scrittura in HDFS è alto, e MapReduce non fornisce metodi per
rielaborare i dati direttamente nella memoria centrale.

Il creatore di Spark, Matei Zaharia{[}@rdd-conf{]}, ha posto questo
problema come dovuto alla mancanza di \emph{primitive efficienti per la
condivisione di dati} in MapReduce. Per come le interfacce di MapReduce
sono poste, sarebbe anche difficile crearne di nuove, data la mancanza
di un'API che sia rappresentativa del dataset invece che delle singole
chiavi e valori.

Infine, la scrittura dei risultati delle computazioni in HDFS è
necessaria per fornire fault-tolerance su di questi, che andrebbero
persi nel caso di un fallimento di un nodo che mantiene i risultati
nella memoria centrale. Un sistema di elaborazione che agisca sulla
memoria centrale deve necessariamente avere un meccanismo di recupero da
fault, per evitare che il fallimento di uno dei singoli nodi coinvolti
nella computazione renda necessario rieseguire completamente
l'applicazione.

Spark si propone come alternativa a MapReduce, con l'intenzione di dare
una soluzione a questi problemi. Le soluzioni derivano da un approccio
funzionale, sfruttando strutture con semantica di immutabilibità per
rappresentare i dataset e API che utilizzano funzioni di ordine
superiore per esprimere concisamente le computazioni. L'astrazione
principale del modello di Spark è il Resilient Distributed Dataset, o
RDD, che rappresenta una collezione immutabile e distribuita di record
di cui è composto un dataset o una sua rielaborazione.

Spark è scritto in Scala, e la sua esecuzione su Hadoop è gestita da
YARN. YARN non è l'unico motore di esecuzione di Spark, che può essere
eseguito anche su Apache Mesos o in modalità standalone, sia su cluster
che su macchine singole. Le API client di Spark sono canonicamente
disponibili in Scala, Java, R e Python.

Spark dispone anche di una modalità interattiva, in cui l'utente
interagisce con il framework tramite una shell REPL Scala o Python.
Questa modalità permette la prototipazione rapida di applicazioni, e
abilita l'utilizzo di paradigmi come l'\textbf{interactive data mining},
che consiste nell'eseguire analisi sui dataset in via esploratoria,
scegliendo quali operazioni intraprendere mano a mano che si riceve il
risultato delle elaborazioni precedenti.

\subsection{Interfaccia di Spark}\label{interfaccia-di-spark}

I Resilient Distributed Dataset sono degli oggetti che rappresentano un
dataset partizionato e distribuito, su cui è possibile eseguire
operazioni parallelamente.

Gli RDD sono immutabili, e ogni computazione richiesta su di questi
restituisce un valore o un nuovo RDD. Le computazioni sono eseguite
tramite metodi chiamati sugli oggetti RDD, e si dividono in due
categorie: \textbf{azioni} e \textbf{trasformazioni}.

Le trasformazioni creano un nuovo RDD, basato su delle operazioni
deterministiche sull'RDD di origine. L'elaborazione del nuovo RDD è
lazy, e non viene eseguita finché non viene richiesta l'esecuzione di
un'azione.

Alcuni esempi di trasformazioni sono \lstinline!map!, che associa a ogni
valore del dataset un nuovo valore, e \lstinline!filter!, che scarta dei
valori nel dataset in base a un predicato. Spesso, per descrivere le
computazioni, le trasformazioni richiedono in input funzioni pure (prive
di side-effect).

Le azioni fanno scattare la valutazione dell'RDD, che porta quindi
all'esecuzione di tutte le trasformazioni da cui questo è derivato.
Alcuni esempi di azioni sono \lstinline!foreach!, che esegue una
funzione specificata dall'utente per ogni input del dataset,
\lstinline!reduce!, che riceve in input una funzione per aggregare i
valori del dataset, e \lstinline!saveAsTextFile!, che permette il
salvataggio di un RDD in un file testuale.

Ogni sessione interattiva e programma Spark utilizza un oggetto
\lstinline!SparkContext! per creare gli RDD iniziali. Lo
\lstinline!SparkContext! contiene le impostazioni principali sul
programma, come il master di esecuzione (\lstinline!local!,
\lstinline!yarn!, \lstinline!mesos!) e l'identificativo con cui
tracciare il job in esecuzione. Le sessioni interattive forniscono lo
\lstinline!SparkContext! automaticamente, in una variabile globale
chiamata \lstinline!sc!.

Le sessioni interattive Spark possono essere avviate tramite i comandi
\lstinline!spark-shell!, che mette a disposizione una shell REPL Scala,
o \lstinline!pyspark!, che ne mette a disposizione una Python. Tramite
gli argomenti dell'eseguibile si può specificare il master (di default
\lstinline!local!).

\begin{figure}
\centering
\includegraphics{img/spark-shell.png}
\caption{Avvio di una sessione interattiva Spark.}
\end{figure}

Tramite l'oggetto \lstinline!sc!, si può creare un nuovo RDD a partire
da diversi fonti:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\item
  Creazione di un RDD a partire da un iterabile:

\begin{lstlisting}[language=Scala]
scala> val range = sc.parallelize(1 to 50)
range: org.apache.spark.rdd.RDD[Int] = 
   ParallelCollectionRDD[0] at parallelize at <console>:24
\end{lstlisting}

  L'espressione \lstinline!1 to 50! rappresenta un range Scala, analogo
  allo stesso concetto in python
\end{enumerate}

Se non vengono fatte specificazioni, la computazione delle
trasformazioni avviene ogni volta che viene chiamata un'azione su di un
RDD. Per evitare ricomputazioni costose, è possibile specificare quali
RDD persistere nella memoria dei nodi, in modo che i risultati computati
possano essere riutilizzati. Per segnalare un RDD come da persistere, è
sufficiente chiamare il suo metodo \lstinline!persist!.

Per ogni RDD Spark è in grado di tracciare tutti gli RDD da cui è
originato, utilizzando un grafo che viene definito \textbf{lineage}.
Tramite questa struttura, Spark è in grado di fornire fault-tolerance:
nell'eventualità in cui un nodo che esegue una computazione su una
partizione dell'RDD dovesse fallire, Spark può retrocedere agli RDD
genitori sul grafo di lineage, fino a trovare un RDD salvato in memoria.
A partire da questo, si può ricavare la partizione del dataset da cui
l'input del nodo fallito è ricavato, e rischedulare la serie di
operazioni per cui la partizione è passata, fino alla rielaborazione
dell'operazione fallita.

Quante più operazioni possibili vengono eseguite nello stesso nodo.
Cambiare i n odi della computazione si rende necessario in certe
operazioni, come l'aggregazione. In questi casi

\begin{longtable}[]{@{}ll@{}}
\caption{Alcune trasformazioni supportate da Spark}\tabularnewline
\toprule
\begin{minipage}[b]{0.34\columnwidth}\raggedright\strut
Trasformazione\strut
\end{minipage} & \begin{minipage}[b]{0.57\columnwidth}\raggedright\strut
Risultato\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.34\columnwidth}\raggedright\strut
Trasformazione\strut
\end{minipage} & \begin{minipage}[b]{0.57\columnwidth}\raggedright\strut
Risultato\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
\lstinline!map(fun)!\strut
\end{minipage} & \begin{minipage}[t]{0.57\columnwidth}\raggedright\strut
Restituisce un nuovo RDD passando ogni elemento della sorgente a
\lstinline!fun!.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
\lstinline!filter(fun)!\strut
\end{minipage} & \begin{minipage}[t]{0.57\columnwidth}\raggedright\strut
Restituisce un RDD formato dagli elementi che \lstinline!fun! mappa in
\lstinline!true!.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
\lstinline!union(dataset)!\strut
\end{minipage} & \begin{minipage}[t]{0.57\columnwidth}\raggedright\strut
Restituisce un RDD che contiene gli elementi della sorgente uniti con
quelli di \lstinline!dataset!.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
\lstinline!intersection(dataset)!\strut
\end{minipage} & \begin{minipage}[t]{0.57\columnwidth}\raggedright\strut
Restitusce un RDD contente gli elementi comuni alla sorgente e a
\lstinline!dataset!\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
\lstinline!distinct([numTasks]))!\strut
\end{minipage} & \begin{minipage}[t]{0.57\columnwidth}\raggedright\strut
Restituisce un RDD contentente gli elementi del dataset senza
ripetizioni\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\chapter{Stream Processing}\label{stream-processing}

\section{Kafka}\label{kafka}

\section{Spark Streaming}\label{spark-streaming}

\section{Storm}\label{storm}

\chapter{Bibliografia}\label{bibliografia}

\end{document}
